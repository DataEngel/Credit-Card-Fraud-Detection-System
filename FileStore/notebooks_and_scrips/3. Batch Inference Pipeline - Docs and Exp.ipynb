{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4354c9cc-756b-473f-a6d8-332c8e1c1c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline para Inferencia Batch en Detección de Fraude\n",
    "\n",
    "## **Descripción**\n",
    "Este pipeline está diseñado para aplicar un modelo preentrenado de detección de fraudes sobre un conjunto de datos transaccionales. Aprovecha PySpark y MLlib para cargar el modelo, generar predicciones y procesar los resultados de manera eficiente. El flujo se ejecuta en **Databricks Community Edition**, aprovechando su entorno escalable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Objetivo**\n",
    "Aplicar un modelo predictivo para clasificar transacciones de tarjetas de crédito como fraudulentas o no fraudulentas, priorizando aquellas de alto riesgo basándose en las probabilidades calculadas.\n",
    "\n",
    "---\n",
    "\n",
    "## **Configuraciones Requeridas**\n",
    "### **Rutas de Entrada y Salida**\n",
    "- **Datos de entrada:** Tabla Delta con características procesadas en `/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf`.\n",
    "- **Modelo preentrenado:** Ruta del modelo en `/dbfs/tmp/fraud_detection_cv_model`.\n",
    "- **Resultados de inferencia:** CSV con transacciones clasificadas en `/FileStore/tables/scored_transactions_results_batch`.\n",
    "- **Etiquetas verdaderas procesadas:** CSV con etiquetas convertidas en `/FileStore/tables/etiquetas_verdaderas_processed`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notas**\n",
    "- Este pipeline está optimizado para manejar grandes volúmenes de datos mediante procesamiento distribuido.\n",
    "- Las rutas configuradas deben ajustarse según el entorno en que se ejecute.\n",
    "- Los resultados permiten priorizar transacciones para investigaciones adicionales y detección temprana de fraudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5ae309-e6da-46c4-bd42-b4dea26aa737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (2.19.0)\r\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (2.0.37)\r\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (3.4.3)\r\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\r\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (7.1.0)\r\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\r\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (3.1.0)\r\nRequirement already satisfied: mlflow-skinny==2.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (2.19.0)\r\nRequirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\r\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (23.0.0)\r\nRequirement already satisfied: markdown<4,>=3.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (3.7)\r\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\r\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\r\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (1.14.1)\r\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\r\nRequirement already satisfied: Jinja2<4,>=2.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow) (3.1.5)\r\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\r\nRequirement already satisfied: requests<3,>=2.17.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\r\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.8)\r\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.19.4)\r\nRequirement already satisfied: cloudpickle<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.1)\r\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\r\nRequirement already satisfied: pyyaml<7,>=5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\r\nRequirement already satisfied: gitpython<4,>=3.1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.44)\r\nRequirement already satisfied: cachetools<6,>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\r\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (21.3)\r\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.41.0)\r\nRequirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\r\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\r\nRequirement already satisfied: google-auth~=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\r\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\r\nRequirement already satisfied: blinker>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from Flask<4->mlflow) (1.9.0)\r\nRequirement already satisfied: Werkzeug>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from Flask<4->mlflow) (3.1.3)\r\nRequirement already satisfied: itsdangerous>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from Flask<4->mlflow) (2.2.0)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.12)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.2)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\r\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\r\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from graphene<4->mlflow) (3.2.5)\r\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from graphene<4->mlflow) (3.2.0)\r\nRequirement already satisfied: zipp>=3.20 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\r\nRequirement already satisfied: deprecated>=1.2.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\r\nRequirement already satisfied: wrapt<2,>=1.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.2)\r\nRequirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\r\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\r\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nRequirement already satisfied: pytest in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (8.3.4)\r\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from pytest) (1.2.2)\r\nRequirement already satisfied: iniconfig in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from pytest) (2.0.0)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.3)\r\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (1.2.2)\r\nRequirement already satisfied: pluggy<2,>=1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/lib/python3.9/site-packages (from pytest) (1.5.0)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-869b5e00-d5b8-446b-936e-d2e934fbed84/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n",
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccea1fe1-c94f-4417-bc40-d92db303c731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Just for explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16ea5d4-4d0c-4d4d-95d3-4eb548bcb87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |merchant_category_count|hour_of_day|day_of_week|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|977424602cfd  |CUST_00000001|2024-08-19 09:48:07.744|193.68567404382148|entertainment    |US              |false       |false   |1724060887       |1                   |193.68567404382148|1                      |9          |2          |\n|14538cde185f  |CUST_00000001|2024-10-03 07:05:48.7  |94.78346493336963 |gas              |US              |true        |false   |1727939148       |1                   |94.78346493336963 |1                      |7          |5          |\n|3ff032257517  |CUST_00000003|2024-10-02 02:04:16.569|171.52404532306835|entertainment    |CA              |false       |false   |1727834656       |1                   |171.52404532306835|1                      |2          |4          |\n|78834b5e5ad9  |CUST_00000003|2024-11-27 10:49:26.553|203.21950168337005|grocery          |US              |true        |false   |1732704566       |1                   |203.21950168337005|2                      |10         |4          |\n|4094d070173f  |CUST_00000003|2024-12-07 04:53:47.216|64.36538011584427 |grocery          |US              |true        |false   |1733547227       |1                   |64.36538011584427 |2                      |4          |7          |\n|8f6180810166  |CUST_00000019|2024-11-29 07:22:37.42 |72.77490776318913 |gas              |US              |true        |false   |1732864957       |1                   |72.77490776318913 |1                      |7          |6          |\n|e14bb5d34230  |CUST_00000019|2024-11-15 19:38:18.921|188.01961993486267|restaurant       |GB              |false       |false   |1731699498       |1                   |188.01961993486267|1                      |19         |6          |\n|89809f754c84  |CUST_00000019|2024-12-18 21:03:41.592|213.41007850987577|retail           |US              |true        |false   |1734555821       |1                   |213.41007850987577|2                      |21         |4          |\n|180594f7ec4e  |CUST_00000019|2025-01-03 06:37:32.414|54.52476215400999 |retail           |US              |true        |false   |1735886252       |1                   |54.52476215400999 |2                      |6          |6          |\n|af9cb8cdd1c0  |CUST_00000031|2024-12-23 23:47:04.046|219.21745181290066|entertainment    |US              |true        |false   |1734997624       |1                   |219.21745181290066|1                      |23         |2          |\n|dbead2c74e26  |CUST_00000031|2024-08-04 22:58:08.123|98.64397348091296 |grocery          |US              |true        |false   |1722812288       |1                   |98.64397348091296 |1                      |22         |1          |\n|69e7b1b7eb43  |CUST_00000062|2024-07-28 01:58:43.576|33.50192889737829 |entertainment    |US              |true        |false   |1722131923       |1                   |33.50192889737829 |1                      |1          |1          |\n|65faa84a083c  |CUST_00000062|2025-01-02 15:05:03.59 |185.31187156019425|grocery          |US              |true        |false   |1735830303       |1                   |185.31187156019425|1                      |15         |5          |\n|b78e73ae06c9  |CUST_00000062|2024-09-23 21:22:17.342|149.01726278321678|retail           |US              |true        |false   |1727126537       |1                   |149.01726278321678|1                      |21         |2          |\n|f9ba4e78b172  |CUST_00000064|2024-09-09 06:38:14.416|99.39580771373589 |entertainment    |US              |false       |false   |1725863894       |1                   |99.39580771373589 |2                      |6          |2          |\n|83fb9020296e  |CUST_00000064|2025-01-02 13:18:10.908|23.766083869698033|entertainment    |US              |true        |false   |1735823890       |1                   |23.766083869698033|2                      |13         |5          |\n|b91f08b4ebaf  |CUST_00000064|2024-11-05 14:18:45.675|101.52952245034382|grocery          |US              |true        |false   |1730816325       |1                   |101.52952245034382|1                      |14         |3          |\n|2ccf144eaa43  |CUST_00000078|2024-12-17 13:27:43.942|150.2796739516982 |entertainment    |US              |false       |false   |1734442063       |1                   |150.2796739516982 |1                      |13         |3          |\n|9493ba1aca16  |CUST_00000078|2024-10-12 22:06:12.878|187.19281842557302|gas              |US              |true        |false   |1728770772       |1                   |187.19281842557302|1                      |22         |7          |\n|fab5b3777f64  |CUST_00000078|2024-12-05 02:24:41.438|164.365345588688  |retail           |US              |false       |false   |1733365481       |1                   |164.365345588688  |1                      |2          |5          |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Delta Table\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ruta del archivo Delta\n",
    "delta_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf\"\n",
    "\n",
    "# Leer el archivo Delta en un DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4463a89f-38e9-438f-b248-f4e7bca6a3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+-----------------+----------------+-------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|summary|transaction_id|  customer_id|            amount|merchant_category|merchant_country|  timestamp_seconds|transaction_velocity|   amount_velocity|merchant_category_count|       hour_of_day|       day_of_week|\n+-------+--------------+-------------+------------------+-----------------+----------------+-------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|  count|        199782|       199782|            199782|           199782|          199782|             199782|              199782|            199782|                 199782|            199782|            199782|\n|   mean|      Infinity|         null|122.62476445443475|             null|            null|1.728391050563289E9|  1.1507743440349982|140.87058286215256|     1.7888298245087144|11.485063719454205| 4.009044858896197|\n| stddev|           NaN|         null|104.94684558552936|             null|            null|  4487326.467556131| 0.38895698463544226|119.06941252502475|     0.8847388815604439| 6.926308967510722|2.0113212092906667|\n|    min|  00000aa7e819|CUST_00000000|1.0196719728417643|    digital_goods|              CA|         1720620215|                   1|1.0196719728417643|                      1|                 0|                 1|\n|    max|  fffff28ca038|CUST_00051010|3995.9535821209215|           travel|              ZZ|         1736172013|                   5|4056.4625875888833|                      7|                23|                 7|\n+-------+--------------+-------------+------------------+-----------------+----------------+-------------------+--------------------+------------------+-----------------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ed83f2-b6b3-4aae-b208-ede3f35cd1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[79]: <bound method DataFrame.printSchema of DataFrame[transaction_id: string, customer_id: string, timestamp: timestamp, amount: double, merchant_category: string, merchant_country: string, card_present: boolean, is_fraud: boolean, timestamp_seconds: bigint, transaction_velocity: bigint, amount_velocity: double, merchant_category_count: bigint, hour_of_day: int, day_of_week: int]>"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd40d0bd-853a-4002-a667-911112f713c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Configuración Inicial: Logging y Sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d584a6eb-b655-4c32-b2a7-15baea2f107f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuración del logger\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "def create_spark_session(app_name=\"Batch Inference Pipeline\"):\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create Spark session: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396d88ea-65f8-4882-9ce5-d6d545efbe39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Model\n",
    "\n",
    "### **Propósito:**\n",
    "Cargar un modelo de regresión logística previamente entrenado desde una ruta especificada.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Se utiliza la función `LogisticRegressionModel.load()` para cargar el modelo desde la ubicación proporcionada.\n",
    "2. Se verifica que la carga sea exitosa y se registra en el logger.\n",
    "\n",
    "### **Output:**\n",
    "Un objeto `LogisticRegressionModel` listo para su uso en tareas de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd082a5-c28d-4882-91a8-39bad0031af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "def load_model(model_path):\n",
    "    try:\n",
    "        logger.info(\"Loading model from: %s\", model_path)\n",
    "        model = LogisticRegressionModel.load(model_path)\n",
    "        logger.info(\"Model loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load model: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35554e9c-2c0b-47dd-8e1b-4a4d440a6993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "### **Propósito:**\n",
    "Cargar datos desde un archivo en formato Delta para realizar tareas de inferencia.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Se utiliza el método `spark.read.format(\"delta\").load()` para leer los datos desde la ruta especificada.\n",
    "2. Se registra en el logger la cantidad de registros cargados para verificar la operación.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame de PySpark con los datos cargados desde el archivo Delta, listo para procesar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313b43b7-5e15-4107-b30b-36aebef36123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar nuevos datos para inferencia\n",
    "def load_data(spark, data_path):\n",
    "    try:\n",
    "        logger.info(\"Loading data from: %s\", data_path)\n",
    "        df = spark.read.format(\"delta\").load(data_path)\n",
    "        logger.info(\"Data loaded successfully. Record count: %d\", df.count())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load data: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481ab5c1-72c9-47ce-b923-284198861309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Features Column\n",
    "\n",
    "### **Propósito:**\n",
    "Generar una columna llamada `features` en el DataFrame que combine las columnas seleccionadas para ser utilizadas como entrada en un modelo de machine learning.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Se inicializa un `VectorAssembler` con las columnas de entrada especificadas en `feature_columns`.\n",
    "2. Se aplica el ensamblador al DataFrame, generando una nueva columna llamada `features` (o el nombre especificado en `output_column`).\n",
    "3. Se registra en el logger la creación exitosa de la columna.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame de PySpark que incluye la columna adicional `features`, lista para ser utilizada en el entrenamiento o inferencia del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad1dace-438d-4279-b5b9-e454f6314425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Crear la columna 'features' en el dataset\n",
    "def create_features_column(df, feature_columns, output_column=\"features\"):\n",
    "    try:\n",
    "        logger.info(\"Creating features column...\")\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=output_column)\n",
    "        df = assembler.transform(df)\n",
    "        logger.info(\"Features column created successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error creating features column: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3365e084-f27f-44e3-bb30-2c9575cd671d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Batch Inference\n",
    "\n",
    "### **Propósito:**\n",
    "Realizar inferencia en lote utilizando un modelo previamente entrenado para generar predicciones sobre un conjunto de datos de entrada.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Se aplica el modelo a los datos de entrada para generar predicciones.\n",
    "2. Se seleccionan columnas clave del DataFrame de predicciones:\n",
    "   - **`transaction_id`**: Identificador único de la transacción.\n",
    "   - **`customer_id`**: Identificador del cliente asociado a la transacción.\n",
    "   - **`features`**: Vector de características utilizado como entrada para el modelo.\n",
    "   - **`probability`**: Vector que contiene las probabilidades de pertenencia a cada clase.\n",
    "   - **`prediction`**: Clase predicha por el modelo (fraude o no fraude).\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame de PySpark que contiene:\n",
    "- **`transaction_id`**\n",
    "- **`customer_id`**\n",
    "- **`features`**\n",
    "- **`probability`**\n",
    "- **`prediction`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3102d74-4441-471c-a16a-6d469821a45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizar inferencia batch\n",
    "def run_batch_inference(model, df):\n",
    "    try:\n",
    "        logger.info(\"Running batch inference...\")\n",
    "        predictions = model.transform(df)\n",
    "        logger.info(\"Inference completed successfully.\")\n",
    "        return predictions.select(\n",
    "            \"transaction_id\", \"customer_id\", \"features\", \"probability\", \"prediction\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during batch inference: %s\", str(e))\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3caa7280-6446-49f9-a54f-fc55f0c2e7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7. Pipeline Principal\n",
    "\n",
    "Este es el flujo principal que llama a todas las funciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d4feb9-cb2e-49c2-b13b-e5025283fa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------------------------------+----------------------------------------+----------+\n|transaction_id|customer_id  |features                             |probability                             |prediction|\n+--------------+-------------+-------------------------------------+----------------------------------------+----------+\n|1043e1069d4c  |CUST_00000006|[1.0,30.36820419587061,1.0,14.0,5.0] |[0.22077749430310337,0.7792225056968967]|1.0       |\n|2b77eb73c4c9  |CUST_00000006|[1.0,216.39150504865475,1.0,8.0,7.0] |[0.19487119565425035,0.8051288043457496]|1.0       |\n|a3c86b26cd1d  |CUST_00000006|[1.0,171.68692778967915,1.0,21.0,1.0]|[0.18726024566166236,0.8127397543383377]|1.0       |\n|5d2e2deaece5  |CUST_00000008|[1.0,123.98424772797799,1.0,17.0,4.0]|[0.2015828235604309,0.7984171764395691] |1.0       |\n|ea8d101644b5  |CUST_00000009|[1.0,98.0001131822433,2.0,22.0,6.0]  |[0.48606146537566997,0.51393853462433]  |1.0       |\n|0ccebeb21ab3  |CUST_00000009|[2.0,275.3706387771217,2.0,1.0,6.0]  |[0.5433010160298879,0.4566989839701121] |0.0       |\n|52e689764776  |CUST_00000009|[1.0,79.73500088328561,1.0,11.0,1.0] |[0.20613807774327628,0.7938619222567237]|1.0       |\n|d0a0e43194f9  |CUST_00000012|[2.0,293.57314533961903,1.0,15.0,2.0]|[0.23041790896922001,0.76958209103078]  |1.0       |\n|7071647cfaf8  |CUST_00000012|[1.0,80.71513949408676,1.0,2.0,5.0]  |[0.2167403907953221,0.783259609204678]  |1.0       |\n|74a82d5afa33  |CUST_00000012|[1.0,135.41014262360665,2.0,9.0,3.0] |[0.4760555923530918,0.5239444076469082] |1.0       |\n|79db59baaa71  |CUST_00000012|[1.0,209.84728342413865,2.0,14.0,3.0]|[0.4538273466362329,0.5461726533637671] |1.0       |\n|c909833119c6  |CUST_00000017|[2.0,221.70739178599877,1.0,8.0,1.0] |[0.2450406429075126,0.7549593570924874] |1.0       |\n|29ca4353405d  |CUST_00000017|[1.0,84.54085357565442,1.0,20.0,4.0] |[0.20708275485706346,0.7929172451429365]|1.0       |\n|aefdbf3d1b97  |CUST_00000017|[1.0,124.91620318894141,1.0,18.0,2.0]|[0.1976050595206247,0.8023949404793753] |1.0       |\n|263cc7f1feb5  |CUST_00000023|[1.0,164.20839255958887,3.0,23.0,6.0]|[0.7608005080819371,0.2391994919180629] |0.0       |\n|b45fc4d0c3ca  |CUST_00000023|[1.0,115.64543925098985,3.0,23.0,2.0]|[0.762148409700641,0.23785159029935898] |0.0       |\n|ebc2aa3dd2aa  |CUST_00000023|[1.0,141.4090263175026,3.0,9.0,6.0]  |[0.7710578576050463,0.2289421423949537] |0.0       |\n|9edcd9fecf88  |CUST_00000023|[1.0,79.88628283945854,1.0,9.0,4.0]  |[0.21224802759720407,0.7877519724027959]|1.0       |\n|a39c99a18f33  |CUST_00000035|[1.0,99.04498922810454,1.0,10.0,4.0] |[0.20854100357274552,0.7914589964272545]|1.0       |\n|774e34b187e7  |CUST_00000038|[1.0,110.88441383611321,1.0,11.0,6.0]|[0.20967096073888397,0.790329039261116] |1.0       |\n+--------------+-------------+-------------------------------------+----------------------------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "def main_pipeline():\n",
    "    try:\n",
    "        logger.info(\"Starting Batch Inference Pipeline...\")\n",
    "\n",
    "        # Configuraciones\n",
    "        data_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf\"\n",
    "        model_path = \"/dbfs/tmp/fraud_detection_cv_model\"\n",
    "\n",
    "        # Crear sesión de Spark\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Cargar datos y modelo\n",
    "        df = load_data(spark, data_path)\n",
    "        model = load_model(model_path)\n",
    "\n",
    "        # Crear la columna 'features' si es necesario\n",
    "        feature_columns = [\n",
    "            \"transaction_velocity\",\n",
    "            \"amount_velocity\",\n",
    "            \"merchant_category_count\",\n",
    "            \"hour_of_day\",\n",
    "            \"day_of_week\"\n",
    "        ]\n",
    "        df = create_features_column(df, feature_columns)\n",
    "\n",
    "        # Realizar inferencia\n",
    "        predictions = run_batch_inference(model, df)\n",
    "\n",
    "        # Mostrar las primeras filas de las predicciones\n",
    "        predictions.show(truncate=False)\n",
    "\n",
    "        logger.info(\"Batch Inference Pipeline completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da1ecd7-2225-4d5a-9c29-1bee70325864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Guardar los resultados en un csv\n",
    "\n",
    "A consecuencia del los tipo de datos complejos, opté por pasarlo a un df de pandas, darle el formato apropiado para el analisis y luego guardarlo como un csv con pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a971cacb-f856-42ff-965a-eaa071b4b809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar los valores 0 y 1 en la columna 'prediction'\n",
    "#predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d5dba6-6ead-4216-903a-c6b1b8040903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------------------------------+----------------------------------------+----------+\n|transaction_id|customer_id  |features                             |probability                             |prediction|\n+--------------+-------------+-------------------------------------+----------------------------------------+----------+\n|1043e1069d4c  |CUST_00000006|[1.0,30.36820419587061,1.0,14.0,5.0] |[0.22077749430310337,0.7792225056968967]|1.0       |\n|2b77eb73c4c9  |CUST_00000006|[1.0,216.39150504865475,1.0,8.0,7.0] |[0.19487119565425035,0.8051288043457496]|1.0       |\n|a3c86b26cd1d  |CUST_00000006|[1.0,171.68692778967915,1.0,21.0,1.0]|[0.18726024566166236,0.8127397543383377]|1.0       |\n|5d2e2deaece5  |CUST_00000008|[1.0,123.98424772797799,1.0,17.0,4.0]|[0.2015828235604309,0.7984171764395691] |1.0       |\n|ea8d101644b5  |CUST_00000009|[1.0,98.0001131822433,2.0,22.0,6.0]  |[0.48606146537566997,0.51393853462433]  |1.0       |\n|0ccebeb21ab3  |CUST_00000009|[2.0,275.3706387771217,2.0,1.0,6.0]  |[0.5433010160298879,0.4566989839701121] |0.0       |\n|52e689764776  |CUST_00000009|[1.0,79.73500088328561,1.0,11.0,1.0] |[0.20613807774327628,0.7938619222567237]|1.0       |\n|d0a0e43194f9  |CUST_00000012|[2.0,293.57314533961903,1.0,15.0,2.0]|[0.23041790896922001,0.76958209103078]  |1.0       |\n|7071647cfaf8  |CUST_00000012|[1.0,80.71513949408676,1.0,2.0,5.0]  |[0.2167403907953221,0.783259609204678]  |1.0       |\n|74a82d5afa33  |CUST_00000012|[1.0,135.41014262360665,2.0,9.0,3.0] |[0.4760555923530918,0.5239444076469082] |1.0       |\n|79db59baaa71  |CUST_00000012|[1.0,209.84728342413865,2.0,14.0,3.0]|[0.4538273466362329,0.5461726533637671] |1.0       |\n|c909833119c6  |CUST_00000017|[2.0,221.70739178599877,1.0,8.0,1.0] |[0.2450406429075126,0.7549593570924874] |1.0       |\n|29ca4353405d  |CUST_00000017|[1.0,84.54085357565442,1.0,20.0,4.0] |[0.20708275485706346,0.7929172451429365]|1.0       |\n|aefdbf3d1b97  |CUST_00000017|[1.0,124.91620318894141,1.0,18.0,2.0]|[0.1976050595206247,0.8023949404793753] |1.0       |\n|263cc7f1feb5  |CUST_00000023|[1.0,164.20839255958887,3.0,23.0,6.0]|[0.7608005080819371,0.2391994919180629] |0.0       |\n|b45fc4d0c3ca  |CUST_00000023|[1.0,115.64543925098985,3.0,23.0,2.0]|[0.762148409700641,0.23785159029935898] |0.0       |\n|ebc2aa3dd2aa  |CUST_00000023|[1.0,141.4090263175026,3.0,9.0,6.0]  |[0.7710578576050463,0.2289421423949537] |0.0       |\n|9edcd9fecf88  |CUST_00000023|[1.0,79.88628283945854,1.0,9.0,4.0]  |[0.21224802759720407,0.7877519724027959]|1.0       |\n|a39c99a18f33  |CUST_00000035|[1.0,99.04498922810454,1.0,10.0,4.0] |[0.20854100357274552,0.7914589964272545]|1.0       |\n|774e34b187e7  |CUST_00000038|[1.0,110.88441383611321,1.0,11.0,6.0]|[0.20967096073888397,0.790329039261116] |1.0       |\n+--------------+-------------+-------------------------------------+----------------------------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting Batch Inference Pipeline...\")\n",
    "\n",
    "# Configuraciones\n",
    "data_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf\"\n",
    "model_path = \"/dbfs/tmp/fraud_detection_cv_model\"\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Cargar datos y modelo\n",
    "df = load_data(spark, data_path)\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Crear la columna 'features' si es necesario\n",
    "feature_columns = [\n",
    "    \"transaction_velocity\",\n",
    "    \"amount_velocity\",\n",
    "    \"merchant_category_count\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "df = create_features_column(df, feature_columns)\n",
    "\n",
    "# Realizar inferencia\n",
    "predictions = run_batch_inference(model, df)\n",
    "\n",
    "# Mostrar las primeras filas de las predicciones\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2d9304-8502-4fd9-9d40-d86cf190f2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[88]: <bound method DataFrame.printSchema of DataFrame[transaction_id: string, customer_id: string, features: vector, probability: vector, prediction: double]>"
     ]
    }
   ],
   "source": [
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a06fd1d-ef8a-4e25-be6e-0d6912cf53f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:122: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Unable to convert the field features. If this column is not necessary, you may consider dropping it or converting to primitive type before the conversion.\nDirect cause: Unsupported type in conversion to Arrow: VectorUDT()\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Convertir el DataFrame PySpark a Pandas\n",
    "predictions = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b65b91-5695-4460-9347-5f12553073a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id    customer_id    prob_0    prob_1  prediction\n0   1043e1069d4c  CUST_00000006  0.220777  0.779223         1.0\n1   2b77eb73c4c9  CUST_00000006  0.194871  0.805129         1.0\n2   a3c86b26cd1d  CUST_00000006  0.187260  0.812740         1.0\n3   5d2e2deaece5  CUST_00000008  0.201583  0.798417         1.0\n4   ea8d101644b5  CUST_00000009  0.486061  0.513939         1.0\n"
     ]
    }
   ],
   "source": [
    "# Separar probabilidades en columnas individuales\n",
    "predictions['prob_0'] = predictions['probability'].apply(lambda x: x[0])  # Probabilidad de no fraude\n",
    "predictions['prob_1'] = predictions['probability'].apply(lambda x: x[1])  # Probabilidad de fraude\n",
    "\n",
    "# Seleccionar las columnas relevantes para el análisis\n",
    "scored_transactions = predictions[['transaction_id', 'customer_id', 'prob_0', 'prob_1', 'prediction']]\n",
    "\n",
    "# Ordenar por probabilidad de fraude (para priorizar transacciones de alto riesgo)\n",
    "#scored_transactions = scored_transactions.sort_values(by='prob_1', ascending=False)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame procesado\n",
    "print(scored_transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b539ab5-0e26-4832-8d23-22ecf2295c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 199782 entries, 0 to 199781\nData columns (total 5 columns):\n #   Column          Non-Null Count   Dtype  \n---  ------          --------------   -----  \n 0   transaction_id  199782 non-null  object \n 1   customer_id     199782 non-null  object \n 2   prob_0          199782 non-null  float64\n 3   prob_1          199782 non-null  float64\n 4   prediction      199782 non-null  float64\ndtypes: float64(3), object(2)\nmemory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ver el esquema del DataFrame en Pandas\n",
    "scored_transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81238c1-8410-4f6e-b8f2-105bcc1058d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored transactions saved successfully to /FileStore/tables/scored_transactions_results_batch\n"
     ]
    }
   ],
   "source": [
    "# 1. Convertir el DataFrame de Pandas a PySpark\n",
    "scored_transactions_spark = spark.createDataFrame(scored_transactions)\n",
    "\n",
    "# 2. Guardar el DataFrame en formato CSV en la ruta de Databricks\n",
    "output_path = \"/FileStore/tables/scored_transactions_results_batch\"\n",
    "\n",
    "scored_transactions_spark.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"Scored transactions saved successfully to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1afa9c-481f-462f-81da-60ee7a7bfa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Leer, formatear y guardas las etiquetas de los verdaderos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c64b64a-7f1e-43a9-bdc0-861bc5dece58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n|transaction_id|is_fraud|\n+--------------+--------+\n|977424602cfd  |false   |\n|14538cde185f  |false   |\n|3ff032257517  |false   |\n|78834b5e5ad9  |false   |\n|4094d070173f  |false   |\n|8f6180810166  |false   |\n|e14bb5d34230  |false   |\n|89809f754c84  |false   |\n|180594f7ec4e  |false   |\n|af9cb8cdd1c0  |false   |\n+--------------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo en formato Delta\n",
    "file_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf\"\n",
    "\n",
    "# Leer el archivo en formato Delta\n",
    "ground_truth = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "# Seleccionar las columnas 'transaction_id' y 'is_fraud'\n",
    "selected_columns = ground_truth.select(\"transaction_id\", \"is_fraud\")\n",
    "\n",
    "# Mostrar las primeras 10 filas\n",
    "selected_columns.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f192e6e8-68b6-4115-9e3e-486a71449b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n|transaction_id|is_fraud|\n+--------------+--------+\n|977424602cfd  |0.0     |\n|14538cde185f  |0.0     |\n|3ff032257517  |0.0     |\n|78834b5e5ad9  |0.0     |\n|4094d070173f  |0.0     |\n|8f6180810166  |0.0     |\n|e14bb5d34230  |0.0     |\n|89809f754c84  |0.0     |\n|180594f7ec4e  |0.0     |\n|af9cb8cdd1c0  |0.0     |\n+--------------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Convertir 'is_fraud' de true/false a 1.0/0.0\n",
    "ground_truth = ground_truth.withColumn(\n",
    "    \"is_fraud\",\n",
    "    when(col(\"is_fraud\") == \"true\", 1.0).when(col(\"is_fraud\") == \"false\", 0.0).otherwise(None)\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas para verificar la conversión\n",
    "ground_truth.select(\"transaction_id\", \"is_fraud\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f4df84-1e07-4265-b9a6-d6562e5e1a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ground truth saved successfully to /FileStore/tables/etiquetas_verdaderas_processed\n"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame procesado en un archivo CSV\n",
    "output_path = \"/FileStore/tables/etiquetas_verdaderas_processed\"\n",
    "\n",
    "# Guardar el DataFrame con las etiquetas convertidas\n",
    "ground_truth.select(\"transaction_id\", \"is_fraud\") \\\n",
    "    .write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"Processed ground truth saved successfully to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ac49e5-ab60-4466-aba6-6f0bd5d7941e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Descripción de las Pruebas\n",
    "\n",
    "### **`test_load_model`**\n",
    "- **Propósito:**  \n",
    "  Valida que la función puede cargar correctamente un modelo guardado en la ruta especificada.\n",
    "\n",
    "---\n",
    "\n",
    "### **`test_load_data`**\n",
    "- **Propósito:**  \n",
    "  Comprueba que la función puede cargar datos desde una tabla Delta.\n",
    "- **Validaciones:**  \n",
    "  - El DataFrame devuelto tiene el esquema esperado.  \n",
    "  - El número de registros coincide con el esperado.\n",
    "\n",
    "---\n",
    "\n",
    "### **`test_create_features_column`**\n",
    "- **Propósito:**  \n",
    "  Asegura que la columna `features` se genere correctamente a partir de las columnas de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### **`test_run_batch_inference`**\n",
    "- **Propósito:**  \n",
    "  Valida que la función puede ejecutar inferencias batch con un modelo de regresión logística.\n",
    "- **Validaciones:**  \n",
    "  - El DataFrame resultante contiene las columnas `prediction` y `probability`.\n",
    "\n",
    "\n",
    "**Nota:** Estas pruebas unitarias, debido a limitaciones de tiempo y a la imposibilidad de ejecutarlas directamente en el mismo notebook, se dejaron de forma hipotética (no las ejecuté como tal, pero aún asi las hice por el requerimiento). Según la metodología de pytest, estas pruebas se ejecutan en archivos `.py`. Por esta razón, decidí priorizar otros pasos del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d883c264-e7bf-4d42-9afc-feefd44240b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"UnitTest\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data(spark):\n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"features\", StringType(), True),\n",
    "        StructField(\"is_fraud\", DoubleType(), True)\n",
    "    ])\n",
    "    data = [\n",
    "        (\"T1\", \"C1\", \"[1.0, 2.0, 3.0]\", 1.0),\n",
    "        (\"T2\", \"C2\", \"[2.0, 3.0, 4.0]\", 0.0),\n",
    "        (\"T3\", \"C3\", \"[3.0, 4.0, 5.0]\", 1.0),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "def test_load_model(tmp_path):\n",
    "    model_path = tmp_path / \"model\"\n",
    "    model = LogisticRegressionModel(uid=\"logistic_regression\")\n",
    "    model.write().overwrite().save(str(model_path))\n",
    "    loaded_model = load_model(str(model_path))\n",
    "    assert isinstance(loaded_model, LogisticRegressionModel)\n",
    "\n",
    "def test_load_data(spark, tmp_path):\n",
    "    data_path = tmp_path / \"data\"\n",
    "    data = [(1, \"C1\"), (2, \"C2\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.write.format(\"delta\").save(str(data_path))\n",
    "    loaded_df = load_data(spark, str(data_path))\n",
    "    assert isinstance(loaded_df, DataFrame)\n",
    "    assert loaded_df.count() == 2\n",
    "\n",
    "def test_create_features_column(sample_data):\n",
    "    feature_columns = [\"transaction_id\", \"customer_id\"]\n",
    "    df = create_features_column(sample_data, feature_columns)\n",
    "    assert \"features\" in df.columns\n",
    "\n",
    "def test_run_batch_inference(spark, sample_data):\n",
    "    model = LogisticRegressionModel(uid=\"logistic_regression\")\n",
    "    model.setFeaturesCol(\"features\").setLabelCol(\"is_fraud\")\n",
    "    model.fit(sample_data)\n",
    "    predictions = run_batch_inference(model, sample_data)\n",
    "    assert \"prediction\" in predictions.columns\n",
    "    assert \"probability\" in predictions.columns\n",
    "    assert predictions.count() == sample_data.count()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Batch Inference Pipeline - Docs and Exp",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
