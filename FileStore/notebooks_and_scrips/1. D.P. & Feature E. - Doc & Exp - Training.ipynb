{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38185a4f-9539-4fa3-b03b-13dc12fd388a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline para Procesamiento e Ingeniería de Características para Detección de Fraude en Transacciones de Tarjetas de Crédito\n",
    "\n",
    "Este notebook implementa un pipeline diseñado para una **institución financiera** que busca construir un sistema de detección de fraudes para identificar transacciones sospechosas con tarjetas de crédito. La solución está optimizada para ejecutarse en **Databricks Community Edition**, aprovechando las capacidades de machine learning (ML) fundamentales mientras se trabaja dentro de las limitaciones de la plataforma.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Propósito General del Notebook\n",
    "1. **Cargar datos crudos:** Importa datos desde un archivo CSV que contiene información de transacciones de tarjetas de crédito.\n",
    "2. **Ingeniería de características:** Realiza las siguientes transformaciones clave:\n",
    "   - **Velocidad de transacciones:** Calcula el número de transacciones realizadas por cliente en una ventana temporal definida.\n",
    "   - **Velocidad de montos:** Calcula el monto total gastado por cliente en la misma ventana temporal.\n",
    "   - **Perfilado de categoría de comerciante:** Analiza las transacciones realizadas en cada categoría comercial para identificar patrones de gasto.\n",
    "   - **Patrones temporales:** Extrae características como la hora del día y el día de la semana para detectar comportamientos o anomalías.\n",
    "3. **Preparación para modelado:** Genera un conjunto de datos enriquecido y listo para entrenar modelos de aprendizaje automático.\n",
    "4. **Almacenamiento en formato Delta:** Guarda los datos procesados en formato **Delta**, optimizado para consultas y análisis en herramientas como Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "## Rutas Necesarias para la Configuración\n",
    "\n",
    "* **Nota:** Las dependecias necesarias para ejecutarlo están el la primera celda \n",
    "\n",
    "### **Archivos de Entrada**\n",
    "- Los datos originales deben subirse en formato CSV (ejemplo: `credit_card_transactions.csv`).\n",
    "- Sube estos archivos a la ruta predeterminada: `/FileStore/tables/` en Databricks.\n",
    "- Ejemplo:\n",
    "\n",
    "/FileStore/tables/credit_card_transactions.csv\n",
    "\n",
    "\n",
    "### **Archivos de Salida**\n",
    "- Los resultados procesados del pipeline se almacenarán en las siguientes rutas:\n",
    "- **Conjunto de entrenamiento:** `/FileStore/tables/credit_card_transactions_dirt_to_train.csv`.\n",
    "- **Conjunto de inferencia:** `/FileStore/tables/credit_card_transactions_dirt_to_inf.csv`.\n",
    "- **Datos enriquecidos en formato Delta:**\n",
    "  ```python\n",
    "  output_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "  save_to_delta(df, output_path)\n",
    "  ```\n",
    "  Este archivo contiene los datos enriquecidos listos para entrenamiento y su almacenamiento en formato Delta asegura compatibilidad y eficiencia para producción.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejecución Completa del Pipeline\n",
    "\n",
    "### 1. **Cargar Datos Originales**\n",
    " - Los datos crudos se cargan desde el archivo CSV ubicado en `/FileStore/tables/credit_card_transactions.csv`.\n",
    "\n",
    "### 2. **Dividir los Datos en Conjuntos**\n",
    " - **80%** para entrenamiento.\n",
    " - **20%** para inferencia.  \n",
    " - Estos conjuntos se guardan como CSV en las rutas mencionadas.\n",
    "\n",
    "### 3. **Aplicar Ingeniería de Características**\n",
    " - **`transaction_velocity`:** Calcula la velocidad de transacciones.\n",
    " - **`amount_velocity`:** Calcula la velocidad de montos.\n",
    " - **`merchant_category_profiling`:** Perfilado de categorías comerciales.\n",
    " - **`time_based_patterns`:** Extrae patrones temporales.\n",
    "\n",
    "### 4. **Guardar Datos Procesados**\n",
    " - Los datos enriquecidos se guardan en formato Delta para modelado:\n",
    "   ```\n",
    "   /FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Contenido del Notebook\n",
    "\n",
    "### **1. Documentación Detallada**\n",
    " - Explicaciones de las transformaciones y métodos aplicados.\n",
    " - Análisis exploratorio inicial de los datos.\n",
    "\n",
    "### **2. Experimentos Relevantes**\n",
    " - Incluye código y pasos intermedios utilizados durante el desarrollo del pipeline, como pruebas de funciones y validaciones.\n",
    "\n",
    "### **3. Código Listo para Producción**\n",
    " - Todas las transformaciones están encapsuladas en funciones reutilizables.\n",
    " - El código final puede trasladarse fácilmente a un **script** o **pipeline de producción** donde sea necesario.\n",
    "\n",
    "---\n",
    "\n",
    "## Pasos para Producción\n",
    "\n",
    "1. **Mover el Código a un Script de Producción**\n",
    " - Extrae las funciones y lógica principales del notebook para integrarlas en un script Python independiente o en un sistema ETL automatizado.\n",
    "\n",
    "2. **Configurar Rutas en el Entorno de Producción**\n",
    " - Asegúrate de que las rutas de entrada y salida estén configuradas correctamente para el entorno donde se ejecutará.\n",
    "\n",
    "3. **Ejecutar en el Entorno Final**\n",
    " - El pipeline está diseñado para trabajar en Databricks, pero puede adaptarse para ejecutarse localmente o en otro entorno Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## Notas Adicionales\n",
    "Este notebook combina documentación, experimentos y un pipeline final listo para su implementación en un entorno de producción o investigación avanzada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee865fa-8af0-4db9-b892-d99f6ab97cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\r\n  Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\r\n\u001B[?25l\r\u001B[K     |█▎                              | 10 kB 31.2 MB/s eta 0:00:01\r\u001B[K     |██▌                             | 20 kB 37.4 MB/s eta 0:00:01\r\u001B[K     |███▉                            | 30 kB 46.2 MB/s eta 0:00:01\r\u001B[K     |█████                           | 40 kB 6.7 MB/s eta 0:00:01\r\u001B[K     |██████▍                         | 51 kB 6.6 MB/s eta 0:00:01\r\u001B[K     |███████▋                        | 61 kB 7.7 MB/s eta 0:00:01\r\u001B[K     |████████▉                       | 71 kB 8.6 MB/s eta 0:00:01\r\u001B[K     |██████████▏                     | 81 kB 9.7 MB/s eta 0:00:01\r\u001B[K     |███████████▍                    | 92 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████▊                   | 102 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 112 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████▏                | 122 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████▌               | 133 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████▊              | 143 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████████             | 153 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████▎           | 163 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▋          | 174 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▉         | 184 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████████        | 194 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▍      | 204 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▋     | 215 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 225 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▏  | 235 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▍ | 245 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▊| 256 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 258 kB 6.9 MB/s \r\n\u001B[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\r\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.7.3)\r\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.21.5)\r\nInstalling collected packages: imbalanced-learn\r\nSuccessfully installed imbalanced-learn-0.12.4\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting mlflow\r\n  Using cached mlflow-2.19.0-py3-none-any.whl (27.4 MB)\r\nCollecting sqlalchemy<3,>=1.4.0\r\n  Using cached SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\nCollecting graphene<4\r\n  Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\r\nCollecting docker<8,>=4.0.0\r\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\r\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\r\nCollecting Flask<4\r\n  Using cached flask-3.1.0-py3-none-any.whl (102 kB)\r\nCollecting mlflow-skinny==2.19.0\r\n  Using cached mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\r\nRequirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\r\nCollecting gunicorn<24\r\n  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\nCollecting markdown<4,>=3.3\r\n  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\r\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\r\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\r\nCollecting alembic!=1.10.0,<2\r\n  Using cached alembic-1.14.1-py3-none-any.whl (233 kB)\r\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\r\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\r\nCollecting sqlparse<1,>=0.4.0\r\n  Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\r\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.27.1)\r\nCollecting opentelemetry-api<3,>=1.9.0\r\n  Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.0.4)\r\nCollecting opentelemetry-sdk<3,>=1.9.0\r\n  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\r\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.19.4)\r\nCollecting cloudpickle<4\r\n  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\nCollecting pyyaml<7,>=5.1\r\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\r\nCollecting gitpython<4,>=3.1.9\r\n  Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\r\nCollecting cachetools<6,>=5.0.0\r\n  Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\r\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (21.3)\r\nCollecting databricks-sdk<1,>=0.20.0\r\n  Using cached databricks_sdk-0.41.0-py3-none-any.whl (637 kB)\r\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\r\nCollecting Mako\r\n  Using cached Mako-1.3.8-py3-none-any.whl (78 kB)\r\nCollecting google-auth~=2.0\r\n  Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\r\nCollecting requests<3,>=2.17.3\r\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\r\nCollecting blinker>=1.9\r\n  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nCollecting Jinja2<4,>=2.11\r\n  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\r\nCollecting Werkzeug>=3.1\r\n  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\nCollecting itsdangerous>=2.2\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nCollecting click<9,>=7.0\r\n  Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\nCollecting gitdb<5,>=4.0.1\r\n  Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\r\nCollecting smmap<6,>=3.0.1\r\n  Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\r\nCollecting rsa<5,>=3.1.4\r\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\r\nCollecting typing-extensions>=4\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\r\nCollecting graphql-core<3.3,>=3.1\r\n  Using cached graphql_core-3.2.5-py3-none-any.whl (203 kB)\r\nCollecting graphql-relay<3.3,>=3.1\r\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\nCollecting zipp>=3.20\r\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\r\nCollecting deprecated>=1.2.6\r\n  Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\r\nCollecting wrapt<2,>=1.10\r\n  Using cached wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\r\nCollecting opentelemetry-semantic-conventions==0.50b0\r\n  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\r\nCollecting pyasn1<0.7.0,>=0.4.6\r\n  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-3.1.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (597 kB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\nInstalling collected packages: zipp, wrapt, pyasn1, importlib-metadata, deprecated, typing-extensions, smmap, rsa, pyasn1-modules, opentelemetry-api, cachetools, requests, opentelemetry-semantic-conventions, MarkupSafe, greenlet, graphql-core, google-auth, gitdb, Werkzeug, sqlparse, sqlalchemy, pyyaml, opentelemetry-sdk, Mako, Jinja2, itsdangerous, graphql-relay, gitpython, databricks-sdk, cloudpickle, click, blinker, mlflow-skinny, markdown, gunicorn, graphene, Flask, docker, alembic, mlflow\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 4.1.1\r\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n  Attempting uninstall: requests\r\n    Found existing installation: requests 2.27.1\r\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'requests'. No files were found to uninstall.\r\n  Attempting uninstall: MarkupSafe\r\n    Found existing installation: MarkupSafe 2.0.1\r\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\r\n  Attempting uninstall: Jinja2\r\n    Found existing installation: Jinja2 2.11.3\r\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\r\n  Attempting uninstall: click\r\n    Found existing installation: click 8.0.4\r\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'click'. No files were found to uninstall.\r\nSuccessfully installed Flask-3.1.0 Jinja2-3.1.5 Mako-1.3.8 MarkupSafe-3.0.2 Werkzeug-3.1.3 alembic-1.14.1 blinker-1.9.0 cachetools-5.5.1 click-8.1.8 cloudpickle-3.1.1 databricks-sdk-0.41.0 deprecated-1.2.15 docker-7.1.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.37.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-8.5.0 itsdangerous-2.2.0 markdown-3.7 mlflow-2.19.0 mlflow-skinny-2.19.0 opentelemetry-api-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 smmap-5.0.2 sqlalchemy-2.0.37 sqlparse-0.5.3 typing-extensions-4.12.2 wrapt-1.17.2 zipp-3.21.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting pytest\r\n  Using cached pytest-8.3.4-py3-none-any.whl (343 kB)\r\nCollecting pytest-cov\r\n  Downloading pytest_cov-6.0.0-py3-none-any.whl (22 kB)\r\nCollecting exceptiongroup>=1.0.0rc8\r\n  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\nCollecting iniconfig\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.3)\r\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (1.2.2)\r\nCollecting pluggy<2,>=1.5\r\n  Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nCollecting coverage[toml]>=7.5\r\n  Downloading coverage-7.6.10-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\r\n\u001B[?25l\r\u001B[K     |█▍                              | 10 kB 7.9 MB/s eta 0:00:01\r\u001B[K     |██▉                             | 20 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |████▏                           | 30 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████▋                          | 40 kB 5.9 MB/s eta 0:00:01\r\u001B[K     |███████                         | 51 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |████████▍                       | 61 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |█████████▊                      | 71 kB 5.8 MB/s eta 0:00:01\r\u001B[K     |███████████▏                    | 81 kB 5.9 MB/s eta 0:00:01\r\u001B[K     |████████████▌                   | 92 kB 6.5 MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 102 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████▎                | 112 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 122 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████              | 133 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████████▌            | 143 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████▉           | 153 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▎         | 163 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▋        | 174 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████       | 184 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▍     | 194 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▉    | 204 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▏  | 215 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▋ | 225 kB 6.9 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 235 kB 6.9 MB/s \r\n\u001B[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\r\nInstalling collected packages: pluggy, iniconfig, exceptiongroup, coverage, pytest, pytest-cov\r\n  Attempting uninstall: pluggy\r\n    Found existing installation: pluggy 1.0.0\r\n    Not uninstalling pluggy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9\r\n    Can't uninstall 'pluggy'. No files were found to uninstall.\r\nSuccessfully installed coverage-7.6.10 exceptiongroup-1.2.2 iniconfig-2.0.0 pluggy-1.5.0 pytest-8.3.4 pytest-cov-6.0.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting pytest-ipynb\r\n  Downloading pytest-ipynb-1.1.1.tar.gz (3.7 kB)\r\nRequirement already satisfied: pytest in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from pytest-ipynb) (8.3.4)\r\nCollecting runipy\r\n  Downloading runipy-0.1.5.tar.gz (24 kB)\r\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from pytest->pytest-ipynb) (1.2.2)\r\nRequirement already satisfied: iniconfig in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from pytest->pytest-ipynb) (2.0.0)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest->pytest-ipynb) (21.3)\r\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest->pytest-ipynb) (1.2.2)\r\nRequirement already satisfied: pluggy<2,>=1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from pytest->pytest-ipynb) (1.5.0)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest->pytest-ipynb) (3.0.4)\r\nRequirement already satisfied: Jinja2>=2.7.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from runipy->pytest-ipynb) (3.1.5)\r\nRequirement already satisfied: Pygments>=1.6 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (2.11.2)\r\nRequirement already satisfied: ipython>=2.3.1 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (8.5.0)\r\nRequirement already satisfied: pyzmq>=14.1.0 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (22.3.0)\r\nRequirement already satisfied: ipykernel>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (6.15.3)\r\nRequirement already satisfied: nbconvert>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (6.4.4)\r\nRequirement already satisfied: nbformat>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from runipy->pytest-ipynb) (5.3.0)\r\nRequirement already satisfied: jupyter-client>=6.1.12 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (6.1.12)\r\nRequirement already satisfied: tornado>=6.1 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (6.1)\r\nRequirement already satisfied: matplotlib-inline>=0.1 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (0.1.2)\r\nRequirement already satisfied: debugpy>=1.0 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (1.5.1)\r\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (5.8.0)\r\nRequirement already satisfied: traitlets>=5.1.0 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (5.1.1)\r\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.0.0->runipy->pytest-ipynb) (1.5.5)\r\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (0.18.1)\r\nRequirement already satisfied: backcall in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (0.2.0)\r\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (5.1.1)\r\nRequirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (3.0.20)\r\nRequirement already satisfied: stack-data in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (0.2.0)\r\nRequirement already satisfied: pickleshare in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (0.7.5)\r\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.9/site-packages (from ipython>=2.3.1->runipy->pytest-ipynb) (4.8.0)\r\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=2.3.1->runipy->pytest-ipynb) (0.8.3)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/lib/python3.9/site-packages (from Jinja2>=2.7.2->runipy->pytest-ipynb) (3.0.2)\r\nRequirement already satisfied: jupyter-core>=4.6.0 in /databricks/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.0.0->runipy->pytest-ipynb) (4.11.2)\r\nRequirement already satisfied: python-dateutil>=2.1 in /databricks/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.0.0->runipy->pytest-ipynb) (2.8.2)\r\nRequirement already satisfied: entrypoints>=0.2.2 in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.4)\r\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (4.1.0)\r\nRequirement already satisfied: testpath in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.5.0)\r\nRequirement already satisfied: pandocfilters>=1.4.1 in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (1.5.0)\r\nRequirement already satisfied: defusedxml in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.7.1)\r\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.5.13)\r\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (4.11.1)\r\nRequirement already satisfied: jupyterlab-pygments in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.1.2)\r\nRequirement already satisfied: mistune<2,>=0.8.1 in /databricks/python3/lib/python3.9/site-packages (from nbconvert>=4.0.0->runipy->pytest-ipynb) (0.8.4)\r\nRequirement already satisfied: fastjsonschema in /databricks/python3/lib/python3.9/site-packages (from nbformat>=4.0.0->runipy->pytest-ipynb) (2.16.2)\r\nRequirement already satisfied: jsonschema>=2.6 in /databricks/python3/lib/python3.9/site-packages (from nbformat>=4.0.0->runipy->pytest-ipynb) (4.4.0)\r\nRequirement already satisfied: attrs>=17.4.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.0.0->runipy->pytest-ipynb) (21.4.0)\r\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.0.0->runipy->pytest-ipynb) (0.18.0)\r\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=2.3.1->runipy->pytest-ipynb) (0.7.0)\r\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=2.3.1->runipy->pytest-ipynb) (0.2.5)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.0.0->runipy->pytest-ipynb) (1.16.0)\r\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=4.0.0->runipy->pytest-ipynb) (2.3.1)\r\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.9/site-packages (from bleach->nbconvert>=4.0.0->runipy->pytest-ipynb) (0.5.1)\r\nRequirement already satisfied: executing in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=2.3.1->runipy->pytest-ipynb) (0.8.3)\r\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=2.3.1->runipy->pytest-ipynb) (0.2.2)\r\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=2.3.1->runipy->pytest-ipynb) (2.0.5)\r\nBuilding wheels for collected packages: pytest-ipynb, runipy\r\n  Building wheel for pytest-ipynb (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n\u001B[?25h  Created wheel for pytest-ipynb: filename=pytest_ipynb-1.1.1-py3-none-any.whl size=4148 sha256=d25898385ee6e9012e97097981f283eefa9b7aa83da501c3ec1086cc76592cf5\r\n  Stored in directory: /root/.cache/pip/wheels/1a/2e/22/ef4bcd703e8240f7dca6ed6f2e1b9c261a9f7a10aaadb7638a\r\n  Building wheel for runipy (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n\u001B[?25h  Created wheel for runipy: filename=runipy-0.1.5-py3-none-any.whl size=10149 sha256=6e6ca653c5d1af52a4aff9ce398ae53ffb06d9755df1abc675448c418752e859\r\n  Stored in directory: /root/.cache/pip/wheels/84/0e/9d/5bd783226ef02a59302f3974ea54e46181a09eec78f67d058c\r\nSuccessfully built pytest-ipynb runipy\r\nInstalling collected packages: runipy, pytest-ipynb\r\nSuccessfully installed pytest-ipynb-1.1.1 runipy-0.1.5\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-591961a2-f31a-465a-8a4f-746b45a920f9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install mlflow\n",
    "!pip install pytest pytest-cov\n",
    "!pip install pytest-ipynb \n",
    "\n",
    "#%fs ls /FileStore/tables/\n",
    "#dbutils.fs.rm(\"/FileStore/tables/engineered_features.delta\", True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e11ae6d-66d8-4091-be78-d3fb362ab5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Initial sampling generation for training and inference \n",
    "\n",
    "* Training (80%): 800,000 records.\n",
    "  * Model building and tuning.\n",
    "* Inference (20%): 200,000 records.\n",
    "  * Data to test the model in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0391c43c-33c3-46ff-bd3b-c9ff23de1f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos guardados: \n- Entrenamiento: /FileStore/tables/credit_card_transactions_dirt_to_train.csv \n- Inferencia: /FileStore/tables/credit_card_transactions_dirt_to_inf.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"SplitDataset\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo original\n",
    "file_path = \"/FileStore/tables/credit_card_transactions.csv\"\n",
    "\n",
    "# Cargar el DataFrame original\n",
    "df_muestreo = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "\n",
    "# Dividir el DataFrame en 80% para entrenamiento y 20% para inferencia\n",
    "train_df, inf_df = df_muestreo.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Guardar el DataFrame de entrenamiento\n",
    "train_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"\n",
    "train_df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(train_path)\n",
    "\n",
    "# Guardar el DataFrame de inferencia\n",
    "inf_path = \"/FileStore/tables/credit_card_transactions_dirt_to_inf.csv\"\n",
    "inf_df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(inf_path)\n",
    "\n",
    "# Confirmación\n",
    "print(f\"Archivos guardados: \\n- Entrenamiento: {train_path} \\n- Inferencia: {inf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8058bf76-6d7d-44f3-9028-79dee3fdb062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Pipeline and Feature Engineering\n",
    "\n",
    "* ### Load and Process the Provided Credit Card Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6036fc-7f75-4ca7-85d2-00a8d2040a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, unix_timestamp, hour, dayofweek\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def create_spark_session(app_name=\"CreditCardFraudDetection\"):\n",
    "    \"\"\"Create and return a Spark session.\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create Spark session: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def load_data(spark, file_path):\n",
    "    \"\"\"Load data from the provided file path.\"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(file_path)\n",
    "        logger.info(\"Data loaded successfully from %s.\", file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load data: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Initial preprocessing: Cast columns to appropriate data types.\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "        logger.info(\"Data preprocessing completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during preprocessing: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b317ef2-2595-4eff-8ba6-af17c1060afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+\n|0000315e14a4  |CUST_00014034|2024-12-11 07:00:53.114|200.84984563271877|retail           |US              |true        |false   |\n|00009fe47aa4  |CUST_00019680|2024-12-11 14:54:16.02 |124.88247268225474|entertainment    |US              |false       |false   |\n|0001407358f4  |CUST_00020433|2024-12-04 00:46:08.626|156.33773977603684|retail           |US              |false       |false   |\n|00018085560f  |CUST_00028904|2024-12-17 17:36:11.758|80.92788170341274 |entertainment    |US              |true        |false   |\n|000345fa005c  |CUST_00032284|2024-12-07 00:55:44.622|128.6288419663436 |gas              |US              |true        |false   |\n|000395afd99e  |CUST_00000434|2024-12-05 11:09:44.975|82.27397303505757 |entertainment    |US              |true        |false   |\n|0005fbabb7d9  |CUST_00030147|2024-12-14 20:28:53.975|145.55795722089445|gas              |DE              |true        |false   |\n|00070c2a8a90  |CUST_00048923|2024-12-11 17:28:25.064|82.02757185707432 |grocery          |US              |false       |false   |\n|00071d1e0cbc  |CUST_00046640|2024-12-05 18:14:03.128|96.40826857531349 |retail           |US              |false       |false   |\n|0007209be82b  |CUST_00000195|2024-12-05 10:22:12.447|106.89213254628922|entertainment    |US              |true        |false   |\n|000786f1ea64  |CUST_00032740|2024-12-07 11:50:36.879|54.301586634366245|entertainment    |US              |true        |false   |\n|0007c52c2272  |CUST_00003659|2024-12-06 11:40:21.492|44.25799362824155 |gas              |US              |true        |false   |\n|000b847d252b  |CUST_00019876|2024-12-15 23:20:31.776|106.33493417627314|restaurant       |US              |true        |false   |\n|000babb14cb2  |CUST_00018418|2024-11-29 19:03:07.228|72.54847687848152 |gas              |US              |true        |false   |\n|000c91362cd2  |CUST_00047069|2024-12-19 22:15:49.366|33.63117890047128 |retail           |US              |true        |false   |\n|000cf05a76bb  |CUST_00011840|2024-12-15 22:06:17.449|49.79584116882301 |retail           |US              |false       |false   |\n|000cfafb5d9b  |CUST_00047410|2024-12-21 07:59:00.096|27.801460080562727|entertainment    |US              |true        |false   |\n|000d55e1a882  |CUST_00025918|2024-11-29 20:05:56.461|78.00301984397845 |restaurant       |US              |true        |false   |\n|000e0e01cbed  |CUST_00028477|2024-11-29 19:16:17.196|72.15341250784186 |gas              |US              |true        |false   |\n|000f59ee2fc0  |CUST_00044465|2024-12-16 10:34:59.294|56.33087196447926 |retail           |US              |true        |false   |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "## Solo para ver la salida de la ejecución\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Ruta del archivo\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"  \n",
    "        \n",
    "        # Cargar datos desde el archivo\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Mostrar el DataFrame original\n",
    "        print(\"DataFrame Original:\")\n",
    "        df.show(20, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319ce45a-27dd-430d-bc04-89da387f7ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: StructType([StructField('transaction_id', StringType(), True), StructField('customer_id', StringType(), True), StructField('timestamp', TimestampType(), True), StructField('amount', DoubleType(), True), StructField('merchant_category', StringType(), True), StructField('merchant_country', StringType(), True), StructField('card_present', BooleanType(), True), StructField('is_fraud', BooleanType(), True)])"
     ]
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc13a3c-0135-4a50-906a-1f828e8259da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+-----------------+----------------+\n|summary|transaction_id|  customer_id|            amount|merchant_category|merchant_country|\n+-------+--------------+-------------+------------------+-----------------+----------------+\n|  count|        800218|       800218|            800218|           800218|          800218|\n|   mean|      Infinity|         null|  123.257890831841|             null|            null|\n| stddev|           NaN|         null|110.76529633203918|             null|            null|\n|    min|  00000a530069|CUST_00000000|1.0036584716915384|    digital_goods|              CA|\n|    max|  ffff74bc4db6|CUST_00051008|  3999.99144862542|           travel|              ZZ|\n+-------+--------------+-------------+------------------+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2faabc-f28a-4769-b442-e34e73718358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análisis de Columnas del DataFrame Original\n",
    "\n",
    "#### 1. `transaction_id`\n",
    "- **Conteo:** 800,218 registros.\n",
    "- **Mínimo y Máximo:** IDs únicos desde `\"00000a530069\"` hasta `\"fffff28ca038\"`.\n",
    "- **Conclusión:** No hay valores faltantes en esta columna, y parece ser un identificador único para cada transacción.\n",
    "\n",
    "#### 2. `customer_id`\n",
    "- **Conteo:** 800,218 registros.\n",
    "- **Rango de IDs:** Desde `\"CUST_00000000\"` hasta `\"CUST_00051010\"`.\n",
    "- **Conclusión:** Todos los registros tienen un cliente asociado, lo que permite agrupar transacciones por cliente para análisis detallado.\n",
    "\n",
    "#### 3. `amount`\n",
    "- **Media:** 123.13 (monto promedio de las transacciones).\n",
    "- **Desviación Estándar:** 109.63 (indica una variabilidad moderada en los montos).\n",
    "- **Mínimo y Máximo:** Transacciones desde 1.00 hasta 3,999.99.\n",
    "- **Conclusión:** Hay una gran dispersión en los montos de transacciones, lo que puede reflejar distintos tipos de compras, desde pequeños gastos hasta compras más grandes.\n",
    "\n",
    "#### 4. `merchant_category`\n",
    "- **Conteo:** 800,218 registros.\n",
    "- **Valores Ejemplo:** Incluye categorías como `\"digital_goods\"`, `\"travel\"`.\n",
    "- **Conclusión:** Todas las transacciones están asociadas a una categoría comercial, lo que permite analizar el comportamiento de gasto por categoría.\n",
    "\n",
    "#### 5. `merchant_country`\n",
    "- **Rango de Países:** Desde `\"CA\"` hasta `\"ZZ\"`.\n",
    "- **Conclusión:** La columna incluye transacciones internacionales, y `\"ZZ\"` podría indicar un valor anómalo o de datos no clasificados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc0f431-b660-495e-bb67-68b0028de435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transaction Velocity\n",
    "\n",
    "### **Propósito:**\n",
    "Calcula el número de transacciones realizadas por cada cliente en los últimos 7 días desde el momento de cada transacción.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Convierte la columna `timestamp` a segundos (`timestamp_seconds`) para facilitar cálculos temporales.\n",
    "2. Define una ventana temporal de 7 días agrupada por `customer_id` y ordenada por tiempo.\n",
    "3. Calcula la cantidad de transacciones en esa ventana y agrega una nueva columna llamada `transaction_velocity`.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`timestamp_seconds`**: La fecha de cada transacción en segundos desde Unix Epoch.\n",
    "- **`transaction_velocity`**: El número de transacciones realizadas por cliente en los últimos 7 días.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4108ffe-7a70-496a-adcb-e3b25cac5dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transaction_velocity(df):\n",
    "    \"\"\"Calculate transaction velocity (number of transactions per time window).\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"timestamp_seconds\", unix_timestamp(col(\"timestamp\")))\n",
    "        time_window = Window.partitionBy(\"customer_id\").orderBy(\"timestamp_seconds\").rangeBetween(-604800, 0)\n",
    "        df = df.withColumn(\"transaction_velocity\", count(\"transaction_id\").over(time_window))\n",
    "        logger.info(\"Transaction velocity calculated.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error calculating transaction velocity: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508c55c6-f954-447d-a004-6f92a09a9aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con Velocidad de Transacciones:\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |\n|397c55413b4e  |CUST_00000003|2024-07-23 09:09:34.079|34.50270848929564 |retail           |US              |false       |false   |1721725774       |1                   |\n|2ec503b61bc2  |CUST_00000003|2024-08-20 00:25:52.069|63.15943526241468 |grocery          |US              |true        |false   |1724113552       |1                   |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Ruta del archivo\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"  \n",
    "        \n",
    "        # Cargar datos desde el archivo\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Aplicar la función transaction_velocity\n",
    "        df_with_velocity = transaction_velocity(df)\n",
    "\n",
    "        # Mostrar el DataFrame con la velocidad de transacciones\n",
    "        print(\"DataFrame con Velocidad de Transacciones:\")\n",
    "        df_with_velocity.show(20, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cc8e9a-07b6-43b2-abbd-022e1f87f4dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Amount Velocity\n",
    "\n",
    "### **Propósito:**\n",
    "Calcula el monto total de las transacciones realizadas por cada cliente en los últimos 7 días desde el momento de cada transacción.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Define una ventana temporal de 7 días agrupada por `customer_id` y ordenada por `timestamp_seconds`.\n",
    "2. Suma los valores de la columna `amount` dentro de la ventana para cada cliente.\n",
    "3. Agrega una nueva columna llamada `amount_velocity` que contiene el monto total calculado.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`amount_velocity`**: El monto total de las transacciones realizadas por cliente en los últimos 7 días.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9988ebd6-5233-444d-a71f-3b0c0cbeb2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def amount_velocity(df):\n",
    "    \"\"\"Calculate amount velocity (total amount per time window).\"\"\"\n",
    "    try:\n",
    "        time_window = Window.partitionBy(\"customer_id\").orderBy(\"timestamp_seconds\").rangeBetween(-604800, 0)\n",
    "        df = df.withColumn(\"amount_velocity\", sum(\"amount\").over(time_window))\n",
    "        logger.info(\"Amount velocity calculated.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error calculating amount velocity: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81766344-b83e-42d6-9a9d-e0cc0edd3258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con Velocidad de Transacciones:\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |\n|397c55413b4e  |CUST_00000003|2024-07-23 09:09:34.079|34.50270848929564 |retail           |US              |false       |false   |1721725774       |1                   |\n|2ec503b61bc2  |CUST_00000003|2024-08-20 00:25:52.069|63.15943526241468 |grocery          |US              |true        |false   |1724113552       |1                   |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+\nonly showing top 20 rows\n\nDataFrame con Velocidad de Montos (Amount Velocity):\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |27.853296584608096|\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |214.7937768362995 |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |375.5041393243305 |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |133.3071594642369 |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |314.31479579044054|\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |351.3893987739623 |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |261.8673154502343 |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |199.90252267797104|\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |50.96910413950451 |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |105.6093877210175 |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |131.49187912453397|\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |63.524711117316016|\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |104.64055511620336|\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |222.75959963533413|\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |121.77472999885579|\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |217.15291879337445|\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |123.28753762891778|\n|397c55413b4e  |CUST_00000003|2024-07-23 09:09:34.079|34.50270848929564 |retail           |US              |false       |false   |1721725774       |1                   |34.50270848929564 |\n|2ec503b61bc2  |CUST_00000003|2024-08-20 00:25:52.069|63.15943526241468 |grocery          |US              |true        |false   |1724113552       |1                   |63.15943526241468 |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |210.51056589211444|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Ruta del archivo\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"\n",
    "        \n",
    "        # Cargar datos desde el archivo\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Aplicar la función transaction_velocity\n",
    "        df_with_velocity = transaction_velocity(df)\n",
    "\n",
    "        # Mostrar el DataFrame con la velocidad de transacciones\n",
    "        print(\"DataFrame con Velocidad de Transacciones:\")\n",
    "        df_with_velocity.show(20, truncate=False)\n",
    "\n",
    "        # Aplicar la función amount_velocity sobre df_with_velocity\n",
    "        df_with_amount_velocity = amount_velocity(df_with_velocity)\n",
    "\n",
    "        # Mostrar el DataFrame con la velocidad de montos\n",
    "        print(\"DataFrame con Velocidad de Montos (Amount Velocity):\")\n",
    "        df_with_amount_velocity.show(20, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b49248a-c551-4439-8a95-c36ed23cc10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merchant Category Profiling\n",
    "\n",
    "### **Propósito:**\n",
    "Calcula el número de transacciones realizadas por cada cliente en una categoría comercial específica.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Define una ventana agrupada por `customer_id` y `merchant_category`.\n",
    "2. Cuenta el número de transacciones (`transaction_id`) dentro de cada categoría para cada cliente.\n",
    "3. Agrega una nueva columna llamada `merchant_category_count` que contiene el número total de transacciones por categoría comercial.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`merchant_category_count`**: El número de transacciones realizadas por cliente en cada categoría comercial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09456c21-7821-46a4-bbfd-d8b31336c8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merchant_category_profiling(df):\n",
    "    \"\"\"Calculate merchant category profiling for each customer.\"\"\"\n",
    "    try:\n",
    "        category_window = Window.partitionBy(\"customer_id\", \"merchant_category\")\n",
    "        df = df.withColumn(\"merchant_category_count\", count(\"transaction_id\").over(category_window))\n",
    "        logger.info(\"Merchant category profiling completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during merchant category profiling: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d01eb59-26af-4c3c-b94e-16ea529fc995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con Perfil de Categorías Comerciales (Merchant Category Profiling):\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |merchant_category_count|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |351.3893987739623 |5                      |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |199.90252267797104|5                      |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |50.96910413950451 |5                      |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |121.77472999885579|5                      |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |217.15291879337445|5                      |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |314.31479579044054|1                      |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |133.3071594642369 |4                      |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |261.8673154502343 |4                      |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |105.6093877210175 |4                      |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |222.75959963533413|4                      |\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |27.853296584608096|4                      |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |375.5041393243305 |4                      |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |63.524711117316016|4                      |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |104.64055511620336|4                      |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |214.7937768362995 |2                      |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |131.49187912453397|2                      |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |123.28753762891778|2                      |\n|992612680713  |CUST_00000003|2024-12-10 10:15:31.498|160.2751150891166 |entertainment    |US              |true        |false   |1733825731       |1                   |160.2751150891166 |2                      |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |210.51056589211444|2                      |\n|3463b54c6d29  |CUST_00000003|2024-10-24 19:22:33.892|75.08163417026941 |gas              |US              |true        |false   |1729797753       |1                   |75.08163417026941 |2                      |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Ruta del archivo\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"\n",
    "        \n",
    "        # Cargar datos desde el archivo\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Aplicar la función transaction_velocity\n",
    "        df_with_velocity = transaction_velocity(df)\n",
    "\n",
    "        # Aplicar la función amount_velocity sobre df_with_velocity\n",
    "        df_with_amount_velocity = amount_velocity(df_with_velocity)\n",
    "\n",
    "        # Aplicar la función merchant_category_profiling\n",
    "        df_with_category_profiling = merchant_category_profiling(df_with_amount_velocity)\n",
    "\n",
    "        # Mostrar el DataFrame con el perfil de categorías comerciales\n",
    "        print(\"DataFrame con Perfil de Categorías Comerciales (Merchant Category Profiling):\")\n",
    "        df_with_category_profiling.show(20, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02edcac-841c-4531-b5aa-3a0d07fa4834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Time-Based Patterns\n",
    "\n",
    "### **Propósito:**\n",
    "Extrae patrones temporales de las transacciones, como la hora del día y el día de la semana, para análisis de comportamiento.\n",
    "\n",
    "### **Proceso:**\n",
    "1. Calcula la hora del día a partir de la columna `timestamp` y la agrega como una nueva columna llamada `hour_of_day`.\n",
    "2. Determina el día de la semana a partir de la columna `timestamp` y lo agrega como una nueva columna llamada `day_of_week`.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`hour_of_day`**: La hora del día en que se realizó la transacción.\n",
    "- **`day_of_week`**: El día de la semana en que ocurrió la transacción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6ad3ac-7672-4841-9ee0-3d90ef2e2c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_based_patterns(df):\n",
    "    \"\"\"Extract time-based patterns like hour of day and day of week.\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"hour_of_day\", hour(col(\"timestamp\")))\n",
    "        df = df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\")))\n",
    "        logger.info(\"Time-based patterns extracted.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error extracting time-based patterns: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21844b9c-ec98-4c3b-b39d-da16ecdb6990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con Patrones Basados en Tiempo (Time-Based Patterns):\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |merchant_category_count|hour_of_day|day_of_week|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |351.3893987739623 |5                      |17         |7          |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |199.90252267797104|5                      |13         |5          |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |50.96910413950451 |5                      |10         |6          |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |121.77472999885579|5                      |18         |6          |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |217.15291879337445|5                      |10         |2          |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |314.31479579044054|1                      |13         |7          |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |133.3071594642369 |4                      |17         |4          |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |261.8673154502343 |4                      |10         |7          |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |105.6093877210175 |4                      |9          |6          |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |222.75959963533413|4                      |11         |3          |\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |27.853296584608096|4                      |3          |6          |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |375.5041393243305 |4                      |18         |7          |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |63.524711117316016|4                      |20         |1          |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |104.64055511620336|4                      |2          |2          |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |214.7937768362995 |2                      |8          |7          |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |131.49187912453397|2                      |7          |6          |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |123.28753762891778|2                      |13         |2          |\n|992612680713  |CUST_00000003|2024-12-10 10:15:31.498|160.2751150891166 |entertainment    |US              |true        |false   |1733825731       |1                   |160.2751150891166 |2                      |10         |3          |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |210.51056589211444|2                      |13         |5          |\n|3463b54c6d29  |CUST_00000003|2024-10-24 19:22:33.892|75.08163417026941 |gas              |US              |true        |false   |1729797753       |1                   |75.08163417026941 |2                      |19         |5          |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Ruta del archivo\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"\n",
    "        \n",
    "        # Cargar datos desde el archivo\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Aplicar la función transaction_velocity\n",
    "        df_with_velocity = transaction_velocity(df)\n",
    "\n",
    "        # Aplicar la función amount_velocity sobre df_with_velocity\n",
    "        df_with_amount_velocity = amount_velocity(df_with_velocity)\n",
    "\n",
    "        # Aplicar la función merchant_category_profiling\n",
    "        df_with_category_profiling = merchant_category_profiling(df_with_amount_velocity)\n",
    "\n",
    "        # Aplicar la función time_based_patterns\n",
    "        df_with_time_patterns = time_based_patterns(df_with_category_profiling)\n",
    "\n",
    "        # Mostrar el DataFrame con patrones basados en tiempo\n",
    "        print(\"DataFrame con Patrones Basados en Tiempo (Time-Based Patterns):\")\n",
    "        df_with_time_patterns.show(20, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb5b55-576b-424a-aefb-14a04d15f733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement Feature Engineering Using Spark SQL and Window Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d242789d-d74b-4cbd-b5b1-11cb18bec842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_feature_engineering(df):\n",
    "    \"\"\"Run all feature engineering steps sequentially.\"\"\"\n",
    "    try:\n",
    "        df = transaction_velocity(df)\n",
    "        df = amount_velocity(df)\n",
    "        df = merchant_category_profiling(df)\n",
    "        df = time_based_patterns(df)\n",
    "        logger.info(\"All feature engineering steps completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during full feature engineering: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca17136-751d-4730-b618-e557917312da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Store Engineered Features in Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90599ca9-f72f-4846-aa54-599bc470aacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_to_delta(df, output_path):\n",
    "    \"\"\"Save processed data to Delta format.\"\"\"\n",
    "    try:\n",
    "        if not output_path:\n",
    "            raise ValueError(\"Output path cannot be empty.\")\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        logger.info(\"Data saved to Delta format at %s.\", output_path)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to save data to Delta: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0cef1a-c993-4462-910f-c7c1aaea4633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Production-Ready Code\n",
    "* Main Function for Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ec7ffe-8b76-42db-bb25-6e44103b1f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Setup\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Load data\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_train.csv\"\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Preprocess data\n",
    "        df = preprocess_data(df)\n",
    "\n",
    "        # Perform feature engineering\n",
    "        df = full_feature_engineering(df)\n",
    "\n",
    "        # Save the processed data\n",
    "        output_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "        save_to_delta(df, output_path)\n",
    "\n",
    "        logger.info(\"Data pipeline completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab950d5-e6ea-40cf-bdaa-cf2c58d66daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pruebas Unitarias para Funciones de Procesamiento\n",
    "\n",
    "## 1. `test_preprocess_data`\n",
    "### **Propósito:**\n",
    "Verificar que `preprocess_data`:\n",
    "- Convierte `amount` a `double` y `timestamp` a `timestamp`.\n",
    "- Conserva las columnas necesarias.\n",
    "\n",
    "### **Validación:**\n",
    "La prueba pasa si las columnas `amount` y `timestamp` están presentes en el DataFrame procesado.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `test_transaction_velocity`\n",
    "### **Propósito:**\n",
    "Confirmar que `transaction_velocity`:\n",
    "- Calcula correctamente la velocidad de transacciones en una ventana de 7 días.\n",
    "- Agrega la columna `transaction_velocity`.\n",
    "\n",
    "### **Validación:**\n",
    "La prueba pasa si el DataFrame resultante contiene la columna `transaction_velocity`.\n",
    "\n",
    "**Nota:** Estas pruebas unitarias, debido a limitaciones de tiempo y a la imposibilidad de ejecutarlas directamente en el mismo notebook, se dejaron de forma hipotética (no las ejecuté como tal, pero aún asi las hice por el requerimiento). Según la metodología de pytest, estas pruebas se ejecutan en archivos `.py`. Por esta razón, decidí priorizar otros pasos del proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653fe569-57af-4aa1-af36-d093f0785a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark para pruebas\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"PytestSparkSession\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Crear un DataFrame de prueba\n",
    "@pytest.fixture\n",
    "def sample_data(spark):\n",
    "    data = [\n",
    "        (\"T1\", \"C1\", \"2025-01-01 10:00:00\", 100.0, \"retail\", \"US\", True),\n",
    "        (\"T2\", \"C1\", \"2025-01-02 11:00:00\", 150.0, \"retail\", \"US\", False),\n",
    "    ]\n",
    "    schema = [\"transaction_id\", \"customer_id\", \"timestamp\", \"amount\", \"merchant_category\", \"merchant_country\", \"card_present\"]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "# Definir las pruebas\n",
    "def test_preprocess_data(sample_data):\n",
    "    df = preprocess_data(sample_data)\n",
    "    assert \"amount\" in df.columns\n",
    "    assert \"timestamp\" in df.columns\n",
    "\n",
    "def test_transaction_velocity(sample_data):\n",
    "    df = preprocess_data(sample_data)\n",
    "    df = transaction_velocity(df)\n",
    "    assert \"transaction_velocity\" in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a741bd8-50be-4f41-a963-d48c4c522a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pytest --maxfail=5 --disable-warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4124cdc-545f-4f9f-9b4f-eb073a228308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pytest --cov=. --cov-report=term-missing\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1902010837772086,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. D.P. & Feature E. - Doc & Exp - Training",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
