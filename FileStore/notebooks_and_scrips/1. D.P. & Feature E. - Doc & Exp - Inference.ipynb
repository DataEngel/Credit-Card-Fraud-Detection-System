{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03c322b-4331-476a-a946-a5e03d921fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline para Ingeniería de Características para Inferencia en Transacciones de Tarjetas de Crédito\n",
    "\n",
    "Este notebook implementa un pipeline de **ingeniería de características** diseñado para preparar un conjunto de datos de transacciones de tarjetas de crédito exclusivamente para tareas de **inferencia**. Está optimizado para ejecutarse en **Databricks Community Edition**, maximizando las capacidades de procesamiento de datos dentro de las limitaciones de la plataforma.\n",
    "\n",
    "---\n",
    "\n",
    "## Propósito General del Notebook\n",
    "\n",
    "1. **Cargar datos crudos:** Importa datos desde un archivo CSV con información de transacciones de tarjetas de crédito.\n",
    "2. **Ingeniería de características:** Realiza transformaciones clave para preparar los datos:\n",
    "   - **Velocidad de transacciones:** Número de transacciones realizadas por cliente en una ventana temporal definida.\n",
    "   - **Velocidad de montos:** Monto total gastado por cliente en la misma ventana temporal.\n",
    "   - **Perfilado de categoría de comerciante:** Análisis de patrones de gasto por categoría comercial.\n",
    "   - **Patrones temporales:** Extracción de características como la hora del día y el día de la semana.\n",
    "3. **Preparación para inferencia:** Genera un conjunto de datos enriquecido listo para ser usado por modelos previamente entrenados.\n",
    "4. **Almacenamiento en formato Delta:** Guarda los datos procesados en formato **Delta**, optimizado para consultas y análisis.\n",
    "\n",
    "---\n",
    "\n",
    "## Rutas Necesarias para la Configuración\n",
    "\n",
    "### **Dependencias**\n",
    "- Todas las dependencias necesarias están instaladas en la primera celda del notebook.\n",
    "\n",
    "### **Archivos de Entrada**\n",
    "- Los datos ya están en CSV por la ejecución del notebook anterior (`credit_card_transactions_dirt_to_inf.csv`).\n",
    "- Ubicación en Databricks: `/FileStore/tables/`.\n",
    "\n",
    "### **Archivos de Salida**\n",
    "- **Datos procesados en formato Delta:** Los datos procesados se encontraran en `/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf` una vez finalizada la ejecución del notebook \n",
    "\n",
    "**Nota:** La escases de doc, unit tests y experimentos en este notebook es porque es igual al notebook anterior solo que cambiando el archivo de salida y entrada. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee865fa-8af0-4db9-b892-d99f6ab97cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\r\n  Using cached imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\r\nRequirement already satisfied: scikit-learn>=1.0.2 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\r\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.7.3)\r\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn) (1.21.5)\r\nInstalling collected packages: imbalanced-learn\r\nSuccessfully installed imbalanced-learn-0.12.4\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting mlflow\r\n  Using cached mlflow-2.19.0-py3-none-any.whl (27.4 MB)\r\nCollecting sqlalchemy<3,>=1.4.0\r\n  Using cached SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\nCollecting graphene<4\r\n  Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\r\nCollecting docker<8,>=4.0.0\r\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\r\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\r\nCollecting Flask<4\r\n  Using cached flask-3.1.0-py3-none-any.whl (102 kB)\r\nCollecting mlflow-skinny==2.19.0\r\n  Using cached mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\r\nRequirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\r\nCollecting gunicorn<24\r\n  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\nCollecting markdown<4,>=3.3\r\n  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\r\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\r\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\r\nCollecting alembic!=1.10.0,<2\r\n  Using cached alembic-1.14.1-py3-none-any.whl (233 kB)\r\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\r\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\r\nCollecting sqlparse<1,>=0.4.0\r\n  Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\r\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.27.1)\r\nCollecting opentelemetry-api<3,>=1.9.0\r\n  Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.0.4)\r\nCollecting opentelemetry-sdk<3,>=1.9.0\r\n  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\r\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.19.4)\r\nCollecting cloudpickle<4\r\n  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\nCollecting pyyaml<7,>=5.1\r\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\r\nCollecting gitpython<4,>=3.1.9\r\n  Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\r\nCollecting cachetools<6,>=5.0.0\r\n  Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\r\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (21.3)\r\nCollecting databricks-sdk<1,>=0.20.0\r\n  Using cached databricks_sdk-0.41.0-py3-none-any.whl (637 kB)\r\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\r\nCollecting Mako\r\n  Using cached Mako-1.3.8-py3-none-any.whl (78 kB)\r\nCollecting google-auth~=2.0\r\n  Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\r\nCollecting requests<3,>=2.17.3\r\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\r\nCollecting blinker>=1.9\r\n  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nCollecting Jinja2<4,>=2.11\r\n  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\r\nCollecting Werkzeug>=3.1\r\n  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\nCollecting itsdangerous>=2.2\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nCollecting click<9,>=7.0\r\n  Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\nCollecting gitdb<5,>=4.0.1\r\n  Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\r\nCollecting smmap<6,>=3.0.1\r\n  Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\r\nCollecting rsa<5,>=3.1.4\r\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\r\nCollecting typing-extensions>=4\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\r\nCollecting graphql-core<3.3,>=3.1\r\n  Using cached graphql_core-3.2.5-py3-none-any.whl (203 kB)\r\nCollecting graphql-relay<3.3,>=3.1\r\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\nCollecting zipp>=3.20\r\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\r\nCollecting deprecated>=1.2.6\r\n  Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\r\nCollecting wrapt<2,>=1.10\r\n  Using cached wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\r\nCollecting opentelemetry-semantic-conventions==0.50b0\r\n  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\r\nCollecting pyasn1<0.7.0,>=0.4.6\r\n  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-3.1.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (597 kB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\nInstalling collected packages: zipp, wrapt, pyasn1, importlib-metadata, deprecated, typing-extensions, smmap, rsa, pyasn1-modules, opentelemetry-api, cachetools, requests, opentelemetry-semantic-conventions, MarkupSafe, greenlet, graphql-core, google-auth, gitdb, Werkzeug, sqlparse, sqlalchemy, pyyaml, opentelemetry-sdk, Mako, Jinja2, itsdangerous, graphql-relay, gitpython, databricks-sdk, cloudpickle, click, blinker, mlflow-skinny, markdown, gunicorn, graphene, Flask, docker, alembic, mlflow\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 4.1.1\r\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06\r\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n  Attempting uninstall: requests\r\n    Found existing installation: requests 2.27.1\r\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06\r\n    Can't uninstall 'requests'. No files were found to uninstall.\r\n  Attempting uninstall: MarkupSafe\r\n    Found existing installation: MarkupSafe 2.0.1\r\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06\r\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\r\n  Attempting uninstall: Jinja2\r\n    Found existing installation: Jinja2 2.11.3\r\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06\r\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\r\n  Attempting uninstall: click\r\n    Found existing installation: click 8.0.4\r\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06\r\n    Can't uninstall 'click'. No files were found to uninstall.\r\nSuccessfully installed Flask-3.1.0 Jinja2-3.1.5 Mako-1.3.8 MarkupSafe-3.0.2 Werkzeug-3.1.3 alembic-1.14.1 blinker-1.9.0 cachetools-5.5.1 click-8.1.8 cloudpickle-3.1.1 databricks-sdk-0.41.0 deprecated-1.2.15 docker-7.1.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.37.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-8.5.0 itsdangerous-2.2.0 markdown-3.7 mlflow-2.19.0 mlflow-skinny-2.19.0 opentelemetry-api-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 smmap-5.0.2 sqlalchemy-2.0.37 sqlparse-0.5.3 typing-extensions-4.12.2 wrapt-1.17.2 zipp-3.21.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-5ca7bea9-b248-4e06-82d0-ce91bb64ef06/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8058bf76-6d7d-44f3-9028-79dee3fdb062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Pipeline and Feature Engineering\n",
    "\n",
    "* ### Load and Process the Provided Credit Card Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6036fc-7f75-4ca7-85d2-00a8d2040a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, unix_timestamp, hour, dayofweek\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def create_spark_session(app_name=\"CreditCardFraudDetection\"):\n",
    "    \"\"\"Create and return a Spark session.\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create Spark session: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def load_data(spark, file_path):\n",
    "    \"\"\"Load data from the provided file path.\"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(file_path)\n",
    "        logger.info(\"Data loaded successfully from %s.\", file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load data: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Initial preprocessing: Cast columns to appropriate data types.\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "        logger.info(\"Data preprocessing completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during preprocessing: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc0f431-b660-495e-bb67-68b0028de435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transaction Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4108ffe-7a70-496a-adcb-e3b25cac5dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transaction_velocity(df):\n",
    "    \"\"\"Calculate transaction velocity (number of transactions per time window).\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"timestamp_seconds\", unix_timestamp(col(\"timestamp\")))\n",
    "        time_window = Window.partitionBy(\"customer_id\").orderBy(\"timestamp_seconds\").rangeBetween(-604800, 0)\n",
    "        df = df.withColumn(\"transaction_velocity\", count(\"transaction_id\").over(time_window))\n",
    "        logger.info(\"Transaction velocity calculated.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error calculating transaction velocity: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb08714-60d1-4d01-8257-9d855712fece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Amount Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9988ebd6-5233-444d-a71f-3b0c0cbeb2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def amount_velocity(df):\n",
    "    \"\"\"Calculate amount velocity (total amount per time window).\"\"\"\n",
    "    try:\n",
    "        time_window = Window.partitionBy(\"customer_id\").orderBy(\"timestamp_seconds\").rangeBetween(-604800, 0)\n",
    "        df = df.withColumn(\"amount_velocity\", sum(\"amount\").over(time_window))\n",
    "        logger.info(\"Amount velocity calculated.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error calculating amount velocity: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b49248a-c551-4439-8a95-c36ed23cc10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Merchant Category Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09456c21-7821-46a4-bbfd-d8b31336c8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merchant_category_profiling(df):\n",
    "    \"\"\"Calculate merchant category profiling for each customer.\"\"\"\n",
    "    try:\n",
    "        category_window = Window.partitionBy(\"customer_id\", \"merchant_category\")\n",
    "        df = df.withColumn(\"merchant_category_count\", count(\"transaction_id\").over(category_window))\n",
    "        logger.info(\"Merchant category profiling completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during merchant category profiling: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02edcac-841c-4531-b5aa-3a0d07fa4834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time-Based Patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6ad3ac-7672-4841-9ee0-3d90ef2e2c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_based_patterns(df):\n",
    "    \"\"\"Extract time-based patterns like hour of day and day of week.\"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(\"hour_of_day\", hour(col(\"timestamp\")))\n",
    "        df = df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\")))\n",
    "        logger.info(\"Time-based patterns extracted.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error extracting time-based patterns: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb5b55-576b-424a-aefb-14a04d15f733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement Feature Engineering Using Spark SQL and Window Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d242789d-d74b-4cbd-b5b1-11cb18bec842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_feature_engineering(df):\n",
    "    \"\"\"Run all feature engineering steps sequentially.\"\"\"\n",
    "    try:\n",
    "        df = transaction_velocity(df)\n",
    "        df = amount_velocity(df)\n",
    "        df = merchant_category_profiling(df)\n",
    "        df = time_based_patterns(df)\n",
    "        logger.info(\"All feature engineering steps completed.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during full feature engineering: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca17136-751d-4730-b618-e557917312da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Store Engineered Features in Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90599ca9-f72f-4846-aa54-599bc470aacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_to_delta(df, output_path):\n",
    "    \"\"\"Save processed data to Delta format.\"\"\"\n",
    "    try:\n",
    "        if not output_path:\n",
    "            raise ValueError(\"Output path cannot be empty.\")\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        logger.info(\"Data saved to Delta format at %s.\", output_path)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to save data to Delta: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0cef1a-c993-4462-910f-c7c1aaea4633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Production-Ready Code\n",
    "* Main Function for Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ec7ffe-8b76-42db-bb25-6e44103b1f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Setup\n",
    "        logger.info(\"Starting the data pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Load data\n",
    "        file_path = \"/FileStore/tables/credit_card_transactions_dirt_to_inf.csv\"  # Replace with your file path\n",
    "        df = load_data(spark, file_path)\n",
    "\n",
    "        # Preprocess data\n",
    "        df = preprocess_data(df)\n",
    "\n",
    "        # Perform feature engineering\n",
    "        df = full_feature_engineering(df)\n",
    "\n",
    "        # Save the processed data\n",
    "        output_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_inf\"\n",
    "        save_to_delta(df, output_path)\n",
    "\n",
    "        logger.info(\"Data pipeline completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1902010837772086,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. D.P. & Feature E. - Doc & Exp - Inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
