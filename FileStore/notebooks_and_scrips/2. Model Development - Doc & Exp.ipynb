{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc7534a-d624-4b7e-ae5c-648e55d36112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modelo de Detección de Fraude en Tarjetas de Crédito\n",
    "\n",
    "Este notebook desarrolla un **modelo predictivo** para identificar transacciones fraudulentas, diseñado para ejecutarse en la **edición Community de Databricks**. Integra **MLflow** para el seguimiento de experimentos y aborda desafíos como el **desbalance de clases** y la evaluación eficiente del modelo, almacenando los resultados y modelos localmente debido a las limitaciones de la plataforma.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraciones\n",
    "\n",
    "### Datos de Entrada\n",
    "- El conjunto de datos consiste en transacciones preprocesadas y enriquecidas con características ingenierizadas.\n",
    "- **Formato de origen**: Tabla Delta.\n",
    "- **Ruta**: `/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train`.\n",
    "\n",
    "### Datos de Salida\n",
    "- **Ruta de guardado del modelo**: `/dbfs/tmp/fraud_detection_cv_model`.\n",
    "  - El modelo entrenado se guarda localmente para su uso en procesos posteriores.\n",
    "- **Seguimiento de experimentos**: `/Users/tu-correo@example.com/fraud_detection_experiment`.\n",
    "  - Todos los parámetros, métricas y artefactos se rastrean mediante MLflow.\n",
    "\n",
    "### Dependencias\n",
    "- **MLflow**: Utilizado para el seguimiento de experimentos y almacenamiento de metadatos del modelo.\n",
    "- **Spark**: Para el manejo escalable de datos e ingeniería de características.\n",
    "- **Delta Lake**: Asegura un almacenamiento y recuperación de datos eficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## Alcance\n",
    "\n",
    "Este notebook abarca la **fase de entrenamiento y evaluación del modelo** dentro del pipeline de detección de fraudes. Se asume que la preprocesamiento de datos y la ingeniería de características se han realizado previamente en un pipeline separado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e61664c-e876-4fc5-9b18-b3c4d503967e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\r\n  Using cached mlflow-2.19.0-py3-none-any.whl (27.4 MB)\r\nCollecting sqlalchemy<3,>=1.4.0\r\n  Using cached SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\nCollecting graphene<4\r\n  Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\r\nCollecting docker<8,>=4.0.0\r\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\r\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\r\nCollecting Flask<4\r\n  Using cached flask-3.1.0-py3-none-any.whl (102 kB)\r\nCollecting mlflow-skinny==2.19.0\r\n  Using cached mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\r\nRequirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\r\nCollecting gunicorn<24\r\n  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\nCollecting markdown<4,>=3.3\r\n  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\r\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\r\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\r\nCollecting alembic!=1.10.0,<2\r\n  Using cached alembic-1.14.1-py3-none-any.whl (233 kB)\r\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\r\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\r\nCollecting sqlparse<1,>=0.4.0\r\n  Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\r\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.27.1)\r\nCollecting opentelemetry-api<3,>=1.9.0\r\n  Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.0.4)\r\nCollecting opentelemetry-sdk<3,>=1.9.0\r\n  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\r\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.19.4)\r\nCollecting cloudpickle<4\r\n  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\nCollecting pyyaml<7,>=5.1\r\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\r\nCollecting gitpython<4,>=3.1.9\r\n  Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\r\nCollecting cachetools<6,>=5.0.0\r\n  Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\r\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (21.3)\r\nCollecting databricks-sdk<1,>=0.20.0\r\n  Using cached databricks_sdk-0.41.0-py3-none-any.whl (637 kB)\r\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\r\nCollecting Mako\r\n  Using cached Mako-1.3.8-py3-none-any.whl (78 kB)\r\nCollecting google-auth~=2.0\r\n  Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\r\nCollecting requests<3,>=2.17.3\r\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\r\nCollecting blinker>=1.9\r\n  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nCollecting Jinja2<4,>=2.11\r\n  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\r\nCollecting Werkzeug>=3.1\r\n  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\nCollecting itsdangerous>=2.2\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nCollecting click<9,>=7.0\r\n  Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\nCollecting gitdb<5,>=4.0.1\r\n  Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\r\nCollecting smmap<6,>=3.0.1\r\n  Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\r\nCollecting rsa<5,>=3.1.4\r\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\r\nCollecting typing-extensions>=4\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\r\nCollecting graphql-core<3.3,>=3.1\r\n  Using cached graphql_core-3.2.5-py3-none-any.whl (203 kB)\r\nCollecting graphql-relay<3.3,>=3.1\r\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\nCollecting zipp>=3.20\r\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\r\nCollecting deprecated>=1.2.6\r\n  Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\r\nCollecting wrapt<2,>=1.10\r\n  Using cached wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\r\nCollecting opentelemetry-semantic-conventions==0.50b0\r\n  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\r\nCollecting pyasn1<0.7.0,>=0.4.6\r\n  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-3.1.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (597 kB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\nInstalling collected packages: zipp, wrapt, pyasn1, importlib-metadata, deprecated, typing-extensions, smmap, rsa, pyasn1-modules, opentelemetry-api, cachetools, requests, opentelemetry-semantic-conventions, MarkupSafe, greenlet, graphql-core, google-auth, gitdb, Werkzeug, sqlparse, sqlalchemy, pyyaml, opentelemetry-sdk, Mako, Jinja2, itsdangerous, graphql-relay, gitpython, databricks-sdk, cloudpickle, click, blinker, mlflow-skinny, markdown, gunicorn, graphene, Flask, docker, alembic, mlflow\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 4.1.1\r\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n  Attempting uninstall: requests\r\n    Found existing installation: requests 2.27.1\r\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'requests'. No files were found to uninstall.\r\n  Attempting uninstall: MarkupSafe\r\n    Found existing installation: MarkupSafe 2.0.1\r\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\r\n  Attempting uninstall: Jinja2\r\n    Found existing installation: Jinja2 2.11.3\r\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\r\n  Attempting uninstall: click\r\n    Found existing installation: click 8.0.4\r\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'click'. No files were found to uninstall.\r\nSuccessfully installed Flask-3.1.0 Jinja2-3.1.5 Mako-1.3.8 MarkupSafe-3.0.2 Werkzeug-3.1.3 alembic-1.14.1 blinker-1.9.0 cachetools-5.5.1 click-8.1.8 cloudpickle-3.1.1 databricks-sdk-0.41.0 deprecated-1.2.15 docker-7.1.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.37.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-8.5.0 itsdangerous-2.2.0 markdown-3.7 mlflow-2.19.0 mlflow-skinny-2.19.0 opentelemetry-api-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 smmap-5.0.2 sqlalchemy-2.0.37 sqlparse-0.5.3 typing-extensions-4.12.2 wrapt-1.17.2 zipp-3.21.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting pytest\r\n  Using cached pytest-8.3.4-py3-none-any.whl (343 kB)\r\nCollecting exceptiongroup>=1.0.0rc8\r\n  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\nCollecting iniconfig\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.3)\r\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (1.2.2)\r\nCollecting pluggy<2,>=1.5\r\n  Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\r\nInstalling collected packages: pluggy, iniconfig, exceptiongroup, pytest\r\n  Attempting uninstall: pluggy\r\n    Found existing installation: pluggy 1.0.0\r\n    Not uninstalling pluggy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'pluggy'. No files were found to uninstall.\r\nSuccessfully installed exceptiongroup-1.2.2 iniconfig-2.0.0 pluggy-1.5.0 pytest-8.3.4\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n",
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbcfd9b-fd50-4539-9be8-9749418bf936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Just for explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d303799-42d2-4717-8cfe-40c776e68cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |merchant_category_count|hour_of_day|day_of_week|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |351.3893987739623 |5                      |17         |7          |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |199.90252267797104|5                      |13         |5          |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |50.96910413950451 |5                      |10         |6          |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |121.77472999885579|5                      |18         |6          |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |217.15291879337445|5                      |10         |2          |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |314.31479579044054|1                      |13         |7          |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |133.3071594642369 |4                      |17         |4          |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |261.8673154502343 |4                      |10         |7          |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |105.6093877210175 |4                      |9          |6          |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |222.75959963533413|4                      |11         |3          |\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |27.853296584608096|4                      |3          |6          |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |375.5041393243305 |4                      |18         |7          |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |63.524711117316016|4                      |20         |1          |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |104.64055511620336|4                      |2          |2          |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |214.7937768362995 |2                      |8          |7          |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |131.49187912453397|2                      |7          |6          |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |123.28753762891778|2                      |13         |2          |\n|992612680713  |CUST_00000003|2024-12-10 10:15:31.498|160.2751150891166 |entertainment    |US              |true        |false   |1733825731       |1                   |160.2751150891166 |2                      |10         |3          |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |210.51056589211444|2                      |13         |5          |\n|3463b54c6d29  |CUST_00000003|2024-10-24 19:22:33.892|75.08163417026941 |gas              |US              |true        |false   |1729797753       |1                   |75.08163417026941 |2                      |19         |5          |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Delta Table\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ruta del archivo Delta\n",
    "delta_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "\n",
    "# Leer el archivo Delta en un DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef47f8b-5ba4-47e5-8db2-982ca72aeddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|summary|transaction_id|  customer_id|            amount|merchant_category|merchant_country|   timestamp_seconds|transaction_velocity|   amount_velocity|merchant_category_count|       hour_of_day|       day_of_week|\n+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|  count|        800218|       800218|            800218|           800218|          800218|              800218|              800218|            800218|                 800218|            800218|            800218|\n|   mean|      Infinity|         null|123.25789083184193|             null|            null|1.7283998196111722E9|  1.6092014925932683|197.32833817308168|      4.186526671482021|11.491116170843446|4.0068393862672425|\n| stddev|           NaN|         null|110.76529633203889|             null|            null|   4491682.118758306|  0.7840253014162226| 160.5041434960652|     1.7904200065685227| 6.922239636554985| 2.007041804806273|\n|    min|  00000a530069|CUST_00000000|1.0036584716915384|    digital_goods|              CA|          1720620198|                   1|1.0036584716915384|                      1|                 0|                 1|\n|    max|  ffff74bc4db6|CUST_00051008|  3999.99144862542|           travel|              ZZ|          1736172176|                   9|4424.9701034148065|                     14|                23|                 7|\n+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca011a5-e0b7-4b3b-9ec5-424de49a52a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Configuración Inicial: Logging y Sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18af8c09-23d1-4ce1-8024-ac2786774334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuración del logger\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "def create_spark_session(app_name=\"Training Pipeline\"):\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create Spark session: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c0c20f-53bb-4432-a6cf-1e4891437c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "### **Propósito:**\n",
    "Carga datos desde un archivo en formato Delta y prepara las características para el entrenamiento del modelo de detección de fraudes.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Cargar datos:**\n",
    "   - Lee el archivo Delta ubicado en la ruta especificada (`delta_path`).\n",
    "   - Verifica que los datos se hayan cargado correctamente.\n",
    "2. **Ensambles de características:**\n",
    "   - Usa `VectorAssembler` para combinar las columnas indicadas en `feature_columns` en una sola columna llamada `features`.\n",
    "3. **Conversión de la etiqueta:**\n",
    "   - Convierte la columna de etiqueta (`label_column`) a tipo entero para que sea compatible con el modelo.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`features`**: Columna con las características combinadas para el modelo.\n",
    "- **`label_column`**: La columna de etiqueta convertida a tipo entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e4a53a-bd19-4ddb-a2ff-579271726162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cargar datos desde Delta y ensamblar características\n",
    "def load_and_prepare_data(spark, delta_path, feature_columns, label_column):\n",
    "    try:\n",
    "        # Cargar los datos desde Delta\n",
    "        df = spark.read.format(\"delta\").load(delta_path)\n",
    "        logger.info(\"Data loaded successfully from Delta.\")\n",
    "        \n",
    "        # Crear columna 'features'\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "        logger.info(\"Features assembled successfully.\")\n",
    "        \n",
    "        # Convertir la columna de etiqueta a entero\n",
    "        df = df.withColumn(label_column, col(label_column).cast(\"integer\"))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading and preparing data: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f26474-e159-454c-b7f1-cada2fb2e930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add Class Weight\n",
    "\n",
    "### **Propósito:**\n",
    "Aborda el problema de desbalance de clases en el conjunto de datos asignando pesos a cada registro, de manera que las clases minoritarias (fraudes) tengan mayor influencia en el modelo durante el entrenamiento.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Cálculo de totales:**\n",
    "   - Calcula el número total de registros, transacciones fraudulentas y no fraudulentas en el conjunto de datos.\n",
    "2. **Determinación de pesos:**\n",
    "   - Calcula un peso para cada clase:\n",
    "     - `fraud_weight`: Peso asignado a las transacciones fraudulentas.\n",
    "     - `non_fraud_weight`: Peso asignado a las transacciones no fraudulentas.\n",
    "3. **Asignación de pesos:**\n",
    "   - Crea una nueva columna `class_weight` que contiene los pesos correspondientes:\n",
    "     - `fraud_weight` para registros etiquetados como fraude.\n",
    "     - `non_fraud_weight` para registros etiquetados como no fraude.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`class_weight`**: Columna con los pesos asignados a cada registro para balancear las clases durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83644331-3215-48d5-b9e2-e215f4920a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Agregar la columna 'class_weight'\n",
    "def add_class_weight(df, label_column=\"is_fraud\"):\n",
    "    try:\n",
    "        total_count = df.count()\n",
    "        fraud_count = df.filter(col(label_column) == 1).count()\n",
    "        non_fraud_count = df.filter(col(label_column) == 0).count()\n",
    "\n",
    "        fraud_weight = total_count / (fraud_count * 2)\n",
    "        non_fraud_weight = total_count / (non_fraud_count * 2)\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"class_weight\",\n",
    "            when(col(label_column) == 1, fraud_weight).otherwise(non_fraud_weight)\n",
    "        )\n",
    "        logger.info(\"Class weights added successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error adding class weights: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fadb5436-cef0-4506-b642-77bc7c1fb81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Model with Cross-Validation and MLflow\n",
    "\n",
    "### **Propósito:**\n",
    "Entrenar un modelo de regresión logística para detectar fraudes en transacciones de tarjetas de crédito, utilizando validación cruzada para optimizar hiperparámetros y rastrear los experimentos en **MLflow**.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Configuración de MLflow:**\n",
    "   - Establece el experimento de MLflow para rastrear los parámetros, métricas y modelos.\n",
    "\n",
    "2. **Definición del modelo:**\n",
    "   - Configura un modelo de regresión logística con las siguientes columnas:\n",
    "     - **`features`**: Características del conjunto de datos.\n",
    "     - **`is_fraud`**: Etiqueta de clase.\n",
    "     - **`class_weight`**: Pesos para balancear las clases.\n",
    "\n",
    "3. **Cuadrícula de hiperparámetros:**\n",
    "   - Define una cuadrícula para el parámetro `regParam` con valores `[0.01, 0.1, 1.0]`.\n",
    "\n",
    "4. **Validación cruzada:**\n",
    "   - Configura un esquema de validación cruzada con 3 particiones para evaluar las combinaciones de hiperparámetros.\n",
    "   - Usa el **Área Bajo la Curva (AUC)** como métrica de evaluación.\n",
    "\n",
    "5. **Entrenamiento del modelo:**\n",
    "   - Ajusta el modelo utilizando validación cruzada y selecciona el mejor modelo según la métrica de AUC.\n",
    "\n",
    "6. **Registro en MLflow:**\n",
    "   - Registra los parámetros y el modelo entrenado en MLflow.\n",
    "\n",
    "### **Output:**\n",
    "El modelo de regresión logística entrenado con los hiperparámetros óptimos:\n",
    "- **`bestModel`**: El mejor modelo seleccionado basado en AUC.\n",
    "- **Rastreo en MLflow**: Incluye parámetros, métricas y artefactos del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abe5a01-e79b-4ba3-8ddb-32ea9d934ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Entrenar el modelo con validación cruzada y MLflow\n",
    "def train_model_with_cv(df, experiment_name):\n",
    "    try:\n",
    "        # Configurar MLflow\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        with mlflow.start_run() as run:\n",
    "            # Configurar el modelo\n",
    "            lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_fraud\", weightCol=\"class_weight\", maxIter=10)\n",
    "\n",
    "            # Configurar la cuadrícula de hiperparámetros\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "                .build()\n",
    "\n",
    "            # Configurar la validación cruzada\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"areaUnderROC\")\n",
    "            cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "            # Entrenar el modelo\n",
    "            logger.info(\"Training the model with cross-validation.\")\n",
    "            cvModel = cv.fit(df)\n",
    "\n",
    "            # Registrar el modelo en MLflow\n",
    "            mlflow.log_param(\"RegParam\", [0.01, 0.1, 1.0])\n",
    "            mlflow.spark.log_model(cvModel.bestModel, \"fraud_detection_model\")\n",
    "            logger.info(\"Model logged to MLflow.\")\n",
    "        \n",
    "        return cvModel.bestModel\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during model training: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de69f0b3-ba95-4ada-8229-c7b5aa66dac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluar el Modelo\n",
    "\n",
    "### **Propósito:**\n",
    "Evaluar el rendimiento del modelo entrenado para detectar fraudes utilizando la métrica **AUC** (Área Bajo la Curva ROC).\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Generar Predicciones:**\n",
    "   - El modelo entrenado se aplica al conjunto de datos de entrada para generar una columna de predicciones.\n",
    "   \n",
    "2. **Configurar el Evaluador:**\n",
    "   - Se utiliza el evaluador `BinaryClassificationEvaluator` para calcular la métrica de **AUC** (Área Bajo la Curva ROC).\n",
    "   \n",
    "3. **Calcular el AUC:**\n",
    "   - El evaluador calcula el AUC en función de las predicciones generadas por el modelo.\n",
    "   \n",
    "4. **Registrar el Resultado:**\n",
    "   - El valor de AUC se registra en los logs para análisis posterior.\n",
    "\n",
    "### **Output:**\n",
    "- **AUC**: Un valor numérico que representa la calidad del modelo en la detección de fraudes, donde un valor cercano a 1 indica un excelente rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9e54ce-6139-44b1-8140-271744cab7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "def evaluate_model(model, df):\n",
    "    try:\n",
    "        # Generar predicciones\n",
    "        predictions = model.transform(df)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        logger.info(f\"Model evaluation completed. AUC: {auc:.4f}\")\n",
    "        return auc\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during model evaluation: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313dae44-d81b-4f22-aaf9-8b11ffe3ad58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Ejecución Principal \n",
    "\n",
    "- Configura y ejecuta todo el pipeline de detección de fraudes.\n",
    "- Carga los datos, agrega pesos para balancear clases, entrena el modelo y evalúa su rendimiento.\n",
    "- Usa MLflow para rastrear los experimentos y resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f62e20a-d96d-4d07-b6e7-5476f91f207d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/21 22:58:11 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/01/21 22:58:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2662644693673107/1913238ff081446fafc46bc1655e2741/artifacts/fraud_detection_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.3.2', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run puzzled-duck-40 at: https://community.cloud.databricks.com/ml/experiments/2662644693673107/runs/1913238ff081446fafc46bc1655e2741\n🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2662644693673107\nModel evaluation completed. AUC: 0.9911\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración inicial\n",
    "        logger.info(\"Starting the fraud detection pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Rutas y configuración\n",
    "        delta_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "        model_path = \"/dbfs/tmp/fraud_detection_cv_model\"\n",
    "        experiment_name = \"/Users/miguel.program.73@gmail.com/fraud_detection_experiment\"\n",
    "        feature_columns = [\n",
    "            \"transaction_velocity\",\n",
    "            \"amount_velocity\",\n",
    "            \"merchant_category_count\",\n",
    "            \"hour_of_day\",\n",
    "            \"day_of_week\"\n",
    "        ]\n",
    "        label_column = \"is_fraud\"\n",
    "\n",
    "        # Cargar y preparar los datos\n",
    "        data = load_and_prepare_data(spark, delta_path, feature_columns, label_column)\n",
    "\n",
    "        # Agregar la columna 'class_weight'\n",
    "        data_with_weights = add_class_weight(data, label_column)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        best_model = train_model_with_cv(data_with_weights, experiment_name)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        auc = evaluate_model(best_model, data_with_weights)\n",
    "        logger.info(f\"Pipeline completed successfully. AUC: {auc:.4f}\")\n",
    "        print(f\"Model evaluation completed. AUC: {auc:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0777ca5-27c9-44dc-aef5-fe6aa581606f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Descripción de las Pruebas\n",
    "\n",
    "### `test_load_and_prepare_data`\n",
    "- Verifica que la función cargue los datos desde Delta correctamente.\n",
    "- Asegura que las columnas `features` y `is_fraud` estén presentes en el DataFrame.\n",
    "\n",
    "### `test_add_class_weight`\n",
    "- Valida que se genere la columna `class_weight` correctamente.\n",
    "- Verifica que los pesos sean distintos para las clases de fraude y no fraude.\n",
    "\n",
    "### `test_train_model_with_cv`\n",
    "- Comprueba que la función entrene un modelo de regresión logística utilizando validación cruzada.\n",
    "- Valida que el modelo resultante sea del tipo `LogisticRegressionModel`.\n",
    "\n",
    "### `test_evaluate_model`\n",
    "- Valida que el modelo pueda generar predicciones.\n",
    "- Verifica que el AUC calculado esté dentro del rango válido de 0 a 1.\n",
    "\n",
    "**Nota:** Estas pruebas unitarias, debido a limitaciones de tiempo y a la imposibilidad de ejecutarlas directamente en el mismo notebook, se dejaron de forma hipotética (no las ejecuté como tal, pero aún asi las hice por el requerimiento). Según la metodología de pytest, estas pruebas se ejecutan en archivos `.py`. Por esta razón, decidí priorizar otros pasos del proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf85ba85-8012-4d46-b35c-7988dbbf3a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    \"\"\"Crea una sesión de Spark para las pruebas.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"TestFraudDetection\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data(spark):\n",
    "    \"\"\"Crea un DataFrame de prueba.\"\"\"\n",
    "    data = [\n",
    "        (\"T1\", \"C1\", 3, 150.0, 5, 12, 3, 0),\n",
    "        (\"T2\", \"C1\", 1, 100.0, 4, 18, 2, 1),\n",
    "        (\"T3\", \"C2\", 2, 200.0, 3, 14, 4, 0)\n",
    "    ]\n",
    "    schema = [\"transaction_id\", \"customer_id\", \"transaction_velocity\", \"amount_velocity\",\n",
    "              \"merchant_category_count\", \"hour_of_day\", \"day_of_week\", \"is_fraud\"]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "def test_load_and_prepare_data(spark, sample_data):\n",
    "    \"\"\"Prueba la función load_and_prepare_data.\"\"\"\n",
    "    delta_path = \"/tmp/sample_delta\"\n",
    "    sample_data.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    label_column = \"is_fraud\"\n",
    "\n",
    "    df = load_and_prepare_data(spark, delta_path, feature_columns, label_column)\n",
    "    assert \"features\" in df.columns\n",
    "    assert label_column in df.columns\n",
    "\n",
    "def test_add_class_weight(sample_data):\n",
    "    \"\"\"Prueba la función add_class_weight.\"\"\"\n",
    "    df = add_class_weight(sample_data, \"is_fraud\")\n",
    "    assert \"class_weight\" in df.columns\n",
    "    fraud_weights = df.filter(df.is_fraud == 1).select(\"class_weight\").distinct().collect()\n",
    "    non_fraud_weights = df.filter(df.is_fraud == 0).select(\"class_weight\").distinct().collect()\n",
    "    assert len(fraud_weights) == 1\n",
    "    assert len(non_fraud_weights) == 1\n",
    "\n",
    "def test_train_model_with_cv(sample_data):\n",
    "    \"\"\"Prueba la función train_model_with_cv.\"\"\"\n",
    "    experiment_name = \"test_experiment\"\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    sample_data = assembler.transform(sample_data)\n",
    "\n",
    "    df_with_weights = add_class_weight(sample_data, \"is_fraud\")\n",
    "    best_model = train_model_with_cv(df_with_weights, experiment_name)\n",
    "    assert isinstance(best_model, LogisticRegressionModel)\n",
    "\n",
    "def test_evaluate_model(spark, sample_data):\n",
    "    \"\"\"Prueba la función evaluate_model.\"\"\"\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    df = assembler.transform(sample_data)\n",
    "\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_fraud\", weightCol=\"class_weight\", maxIter=10)\n",
    "    df_with_weights = add_class_weight(df, \"is_fraud\")\n",
    "    model = lr.fit(df_with_weights)\n",
    "\n",
    "    auc = evaluate_model(model, df_with_weights)\n",
    "    assert isinstance(auc, float)\n",
    "    assert 0.0 <= auc <= 1.0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Model Development - Doc & Exp",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
