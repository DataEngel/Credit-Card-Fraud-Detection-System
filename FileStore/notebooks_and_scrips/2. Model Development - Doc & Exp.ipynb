{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc7534a-d624-4b7e-ae5c-648e55d36112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modelo de Detecci贸n de Fraude en Tarjetas de Cr茅dito\n",
    "\n",
    "Este notebook desarrolla un **modelo predictivo** para identificar transacciones fraudulentas, dise帽ado para ejecutarse en la **edici贸n Community de Databricks**. Integra **MLflow** para el seguimiento de experimentos y aborda desaf铆os como el **desbalance de clases** y la evaluaci贸n eficiente del modelo, almacenando los resultados y modelos localmente debido a las limitaciones de la plataforma.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraciones\n",
    "\n",
    "### Datos de Entrada\n",
    "- El conjunto de datos consiste en transacciones preprocesadas y enriquecidas con caracter铆sticas ingenierizadas.\n",
    "- **Formato de origen**: Tabla Delta.\n",
    "- **Ruta**: `/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train`.\n",
    "\n",
    "### Datos de Salida\n",
    "- **Ruta de guardado del modelo**: `/dbfs/tmp/fraud_detection_cv_model`.\n",
    "  - El modelo entrenado se guarda localmente para su uso en procesos posteriores.\n",
    "- **Seguimiento de experimentos**: `/Users/tu-correo@example.com/fraud_detection_experiment`.\n",
    "  - Todos los par谩metros, m茅tricas y artefactos se rastrean mediante MLflow.\n",
    "\n",
    "### Dependencias\n",
    "- **MLflow**: Utilizado para el seguimiento de experimentos y almacenamiento de metadatos del modelo.\n",
    "- **Spark**: Para el manejo escalable de datos e ingenier铆a de caracter铆sticas.\n",
    "- **Delta Lake**: Asegura un almacenamiento y recuperaci贸n de datos eficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## Alcance\n",
    "\n",
    "Este notebook abarca la **fase de entrenamiento y evaluaci贸n del modelo** dentro del pipeline de detecci贸n de fraudes. Se asume que la preprocesamiento de datos y la ingenier铆a de caracter铆sticas se han realizado previamente en un pipeline separado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e61664c-e876-4fc5-9b18-b3c4d503967e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\r\n  Using cached mlflow-2.19.0-py3-none-any.whl (27.4 MB)\r\nCollecting sqlalchemy<3,>=1.4.0\r\n  Using cached SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\nCollecting graphene<4\r\n  Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\r\nCollecting docker<8,>=4.0.0\r\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\r\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\r\nCollecting Flask<4\r\n  Using cached flask-3.1.0-py3-none-any.whl (102 kB)\r\nCollecting mlflow-skinny==2.19.0\r\n  Using cached mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\r\nRequirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\r\nCollecting gunicorn<24\r\n  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\nCollecting markdown<4,>=3.3\r\n  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\r\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\r\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\r\nCollecting alembic!=1.10.0,<2\r\n  Using cached alembic-1.14.1-py3-none-any.whl (233 kB)\r\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\r\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\r\nCollecting sqlparse<1,>=0.4.0\r\n  Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\r\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.27.1)\r\nCollecting opentelemetry-api<3,>=1.9.0\r\n  Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.0.4)\r\nCollecting opentelemetry-sdk<3,>=1.9.0\r\n  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\r\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.19.4)\r\nCollecting cloudpickle<4\r\n  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\nCollecting pyyaml<7,>=5.1\r\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\r\nCollecting gitpython<4,>=3.1.9\r\n  Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\r\nCollecting cachetools<6,>=5.0.0\r\n  Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\r\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.19.0->mlflow) (21.3)\r\nCollecting databricks-sdk<1,>=0.20.0\r\n  Using cached databricks_sdk-0.41.0-py3-none-any.whl (637 kB)\r\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\r\nCollecting Mako\r\n  Using cached Mako-1.3.8-py3-none-any.whl (78 kB)\r\nCollecting google-auth~=2.0\r\n  Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\r\nCollecting requests<3,>=2.17.3\r\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\r\nCollecting blinker>=1.9\r\n  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nCollecting Jinja2<4,>=2.11\r\n  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\r\nCollecting Werkzeug>=3.1\r\n  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\nCollecting itsdangerous>=2.2\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nCollecting click<9,>=7.0\r\n  Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\nCollecting gitdb<5,>=4.0.1\r\n  Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\r\nCollecting smmap<6,>=3.0.1\r\n  Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\r\nCollecting rsa<5,>=3.1.4\r\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\r\nCollecting typing-extensions>=4\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\r\nCollecting graphql-core<3.3,>=3.1\r\n  Using cached graphql_core-3.2.5-py3-none-any.whl (203 kB)\r\nCollecting graphql-relay<3.3,>=3.1\r\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\nCollecting zipp>=3.20\r\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\r\nCollecting deprecated>=1.2.6\r\n  Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\r\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\r\n  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\r\nCollecting wrapt<2,>=1.10\r\n  Using cached wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\r\nCollecting opentelemetry-semantic-conventions==0.50b0\r\n  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\r\nCollecting pyasn1<0.7.0,>=0.4.6\r\n  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-3.1.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (597 kB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\nInstalling collected packages: zipp, wrapt, pyasn1, importlib-metadata, deprecated, typing-extensions, smmap, rsa, pyasn1-modules, opentelemetry-api, cachetools, requests, opentelemetry-semantic-conventions, MarkupSafe, greenlet, graphql-core, google-auth, gitdb, Werkzeug, sqlparse, sqlalchemy, pyyaml, opentelemetry-sdk, Mako, Jinja2, itsdangerous, graphql-relay, gitpython, databricks-sdk, cloudpickle, click, blinker, mlflow-skinny, markdown, gunicorn, graphene, Flask, docker, alembic, mlflow\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 4.1.1\r\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n  Attempting uninstall: requests\r\n    Found existing installation: requests 2.27.1\r\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'requests'. No files were found to uninstall.\r\n  Attempting uninstall: MarkupSafe\r\n    Found existing installation: MarkupSafe 2.0.1\r\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\r\n  Attempting uninstall: Jinja2\r\n    Found existing installation: Jinja2 2.11.3\r\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\r\n  Attempting uninstall: click\r\n    Found existing installation: click 8.0.4\r\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'click'. No files were found to uninstall.\r\nSuccessfully installed Flask-3.1.0 Jinja2-3.1.5 Mako-1.3.8 MarkupSafe-3.0.2 Werkzeug-3.1.3 alembic-1.14.1 blinker-1.9.0 cachetools-5.5.1 click-8.1.8 cloudpickle-3.1.1 databricks-sdk-0.41.0 deprecated-1.2.15 docker-7.1.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.37.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-8.5.0 itsdangerous-2.2.0 markdown-3.7 mlflow-2.19.0 mlflow-skinny-2.19.0 opentelemetry-api-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 smmap-5.0.2 sqlalchemy-2.0.37 sqlparse-0.5.3 typing-extensions-4.12.2 wrapt-1.17.2 zipp-3.21.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting pytest\r\n  Using cached pytest-8.3.4-py3-none-any.whl (343 kB)\r\nCollecting exceptiongroup>=1.0.0rc8\r\n  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\nCollecting iniconfig\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.3)\r\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (1.2.2)\r\nCollecting pluggy<2,>=1.5\r\n  Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\r\nInstalling collected packages: pluggy, iniconfig, exceptiongroup, pytest\r\n  Attempting uninstall: pluggy\r\n    Found existing installation: pluggy 1.0.0\r\n    Not uninstalling pluggy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654\r\n    Can't uninstall 'pluggy'. No files were found to uninstall.\r\nSuccessfully installed exceptiongroup-1.2.2 iniconfig-2.0.0 pluggy-1.5.0 pytest-8.3.4\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ca27cbfa-2a2b-427c-84c6-1b8ee8e05654/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n",
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbcfd9b-fd50-4539-9be8-9749418bf936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Just for explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d303799-42d2-4717-8cfe-40c776e68cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|transaction_id|customer_id  |timestamp              |amount            |merchant_category|merchant_country|card_present|is_fraud|timestamp_seconds|transaction_velocity|amount_velocity   |merchant_category_count|hour_of_day|day_of_week|\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\n|17286088af61  |CUST_00000001|2024-09-21 17:06:23.596|37.07460298352177 |entertainment    |US              |true        |false   |1726938383       |3                   |351.3893987739623 |5                      |17         |7          |\n|cac80a102495  |CUST_00000001|2024-10-31 13:20:01.202|199.90252267797104|entertainment    |US              |true        |false   |1730380801       |1                   |199.90252267797104|5                      |13         |5          |\n|25c41c405465  |CUST_00000001|2024-11-08 10:46:29.622|50.96910413950451 |entertainment    |US              |false       |false   |1731062789       |1                   |50.96910413950451 |5                      |10         |6          |\n|96ac4e19ab6c  |CUST_00000001|2025-01-03 18:40:38.706|121.77472999885579|entertainment    |US              |true        |false   |1735929638       |1                   |121.77472999885579|5                      |18         |6          |\n|8089d7605cdc  |CUST_00000001|2025-01-06 10:51:11.239|95.37818879451865 |entertainment    |US              |false       |false   |1736160671       |2                   |217.15291879337445|5                      |10         |2          |\n|2e5704009d1d  |CUST_00000001|2024-09-21 13:44:09.939|181.00763632620365|gas              |US              |true        |false   |1726926249       |2                   |314.31479579044054|1                      |13         |7          |\n|431800d6c0b8  |CUST_00000001|2024-09-18 17:59:08.475|133.3071594642369 |grocery          |US              |true        |false   |1726682348       |1                   |133.3071594642369 |4                      |17         |4          |\n|3b16cc3f6971  |CUST_00000001|2024-09-28 10:04:13.506|43.78507614050889 |grocery          |US              |true        |false   |1727517853       |3                   |261.8673154502343 |4                      |10         |7          |\n|883e50be7625  |CUST_00000001|2024-11-22 09:55:05.667|105.6093877210175 |grocery          |US              |true        |false   |1732269305       |1                   |105.6093877210175 |4                      |9          |6          |\n|521527bf1e3f  |CUST_00000001|2024-12-24 11:20:57.876|118.11904451913077|grocery          |US              |false       |false   |1735039257       |3                   |222.75959963533413|4                      |11         |3          |\n|f3520fbdab25  |CUST_00000001|2024-07-26 03:50:59.693|27.853296584608096|restaurant       |US              |true        |false   |1721965859       |1                   |27.853296584608096|4                      |3          |6          |\n|acecb0a89e73  |CUST_00000001|2024-09-07 18:30:41.955|160.71036248803097|restaurant       |US              |false       |false   |1725733841       |2                   |375.5041393243305 |4                      |18         |7          |\n|18276227a494  |CUST_00000001|2024-12-22 20:37:08.518|63.524711117316016|restaurant       |US              |true        |false   |1734899828       |1                   |63.524711117316016|4                      |20         |1          |\n|371d07e33364  |CUST_00000001|2024-12-23 02:11:37.01 |41.11584399888735 |restaurant       |US              |true        |false   |1734919897       |2                   |104.64055511620336|4                      |2          |2          |\n|2c08d3d33433  |CUST_00000001|2024-09-07 08:41:37.913|214.7937768362995 |retail           |US              |false       |false   |1725698497       |1                   |214.7937768362995 |2                      |8          |7          |\n|105bdcc07fba  |CUST_00000001|2024-12-13 07:51:09.953|131.49187912453397|retail           |CA              |false       |false   |1734076269       |1                   |131.49187912453397|2                      |7          |6          |\n|9f6d060e6b4d  |CUST_00000003|2024-07-15 13:50:58.738|123.28753762891778|entertainment    |US              |true        |false   |1721051458       |1                   |123.28753762891778|2                      |13         |2          |\n|992612680713  |CUST_00000003|2024-12-10 10:15:31.498|160.2751150891166 |entertainment    |US              |true        |false   |1733825731       |1                   |160.2751150891166 |2                      |10         |3          |\n|2b3411abee35  |CUST_00000003|2024-08-22 13:57:18.91 |147.35113062969975|gas              |US              |false       |false   |1724335038       |2                   |210.51056589211444|2                      |13         |5          |\n|3463b54c6d29  |CUST_00000003|2024-10-24 19:22:33.892|75.08163417026941 |gas              |US              |true        |false   |1729797753       |1                   |75.08163417026941 |2                      |19         |5          |\n+--------------+-------------+-----------------------+------------------+-----------------+----------------+------------+--------+-----------------+--------------------+------------------+-----------------------+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesi贸n de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Delta Table\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ruta del archivo Delta\n",
    "delta_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "\n",
    "# Leer el archivo Delta en un DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef47f8b-5ba4-47e5-8db2-982ca72aeddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|summary|transaction_id|  customer_id|            amount|merchant_category|merchant_country|   timestamp_seconds|transaction_velocity|   amount_velocity|merchant_category_count|       hour_of_day|       day_of_week|\n+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n|  count|        800218|       800218|            800218|           800218|          800218|              800218|              800218|            800218|                 800218|            800218|            800218|\n|   mean|      Infinity|         null|123.25789083184193|             null|            null|1.7283998196111722E9|  1.6092014925932683|197.32833817308168|      4.186526671482021|11.491116170843446|4.0068393862672425|\n| stddev|           NaN|         null|110.76529633203889|             null|            null|   4491682.118758306|  0.7840253014162226| 160.5041434960652|     1.7904200065685227| 6.922239636554985| 2.007041804806273|\n|    min|  00000a530069|CUST_00000000|1.0036584716915384|    digital_goods|              CA|          1720620198|                   1|1.0036584716915384|                      1|                 0|                 1|\n|    max|  ffff74bc4db6|CUST_00051008|  3999.99144862542|           travel|              ZZ|          1736172176|                   9|4424.9701034148065|                     14|                23|                 7|\n+-------+--------------+-------------+------------------+-----------------+----------------+--------------------+--------------------+------------------+-----------------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca011a5-e0b7-4b3b-9ec5-424de49a52a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Configuraci贸n Inicial: Logging y Sesi贸n de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18af8c09-23d1-4ce1-8024-ac2786774334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuraci贸n del logger\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Crear una sesi贸n de Spark\n",
    "def create_spark_session(app_name=\"Training Pipeline\"):\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Spark session created successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create Spark session: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c0c20f-53bb-4432-a6cf-1e4891437c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "### **Prop贸sito:**\n",
    "Carga datos desde un archivo en formato Delta y prepara las caracter铆sticas para el entrenamiento del modelo de detecci贸n de fraudes.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Cargar datos:**\n",
    "   - Lee el archivo Delta ubicado en la ruta especificada (`delta_path`).\n",
    "   - Verifica que los datos se hayan cargado correctamente.\n",
    "2. **Ensambles de caracter铆sticas:**\n",
    "   - Usa `VectorAssembler` para combinar las columnas indicadas en `feature_columns` en una sola columna llamada `features`.\n",
    "3. **Conversi贸n de la etiqueta:**\n",
    "   - Convierte la columna de etiqueta (`label_column`) a tipo entero para que sea compatible con el modelo.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`features`**: Columna con las caracter铆sticas combinadas para el modelo.\n",
    "- **`label_column`**: La columna de etiqueta convertida a tipo entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e4a53a-bd19-4ddb-a2ff-579271726162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cargar datos desde Delta y ensamblar caracter铆sticas\n",
    "def load_and_prepare_data(spark, delta_path, feature_columns, label_column):\n",
    "    try:\n",
    "        # Cargar los datos desde Delta\n",
    "        df = spark.read.format(\"delta\").load(delta_path)\n",
    "        logger.info(\"Data loaded successfully from Delta.\")\n",
    "        \n",
    "        # Crear columna 'features'\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "        logger.info(\"Features assembled successfully.\")\n",
    "        \n",
    "        # Convertir la columna de etiqueta a entero\n",
    "        df = df.withColumn(label_column, col(label_column).cast(\"integer\"))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading and preparing data: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f26474-e159-454c-b7f1-cada2fb2e930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add Class Weight\n",
    "\n",
    "### **Prop贸sito:**\n",
    "Aborda el problema de desbalance de clases en el conjunto de datos asignando pesos a cada registro, de manera que las clases minoritarias (fraudes) tengan mayor influencia en el modelo durante el entrenamiento.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **C谩lculo de totales:**\n",
    "   - Calcula el n煤mero total de registros, transacciones fraudulentas y no fraudulentas en el conjunto de datos.\n",
    "2. **Determinaci贸n de pesos:**\n",
    "   - Calcula un peso para cada clase:\n",
    "     - `fraud_weight`: Peso asignado a las transacciones fraudulentas.\n",
    "     - `non_fraud_weight`: Peso asignado a las transacciones no fraudulentas.\n",
    "3. **Asignaci贸n de pesos:**\n",
    "   - Crea una nueva columna `class_weight` que contiene los pesos correspondientes:\n",
    "     - `fraud_weight` para registros etiquetados como fraude.\n",
    "     - `non_fraud_weight` para registros etiquetados como no fraude.\n",
    "\n",
    "### **Output:**\n",
    "Un DataFrame que incluye:\n",
    "- **`class_weight`**: Columna con los pesos asignados a cada registro para balancear las clases durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83644331-3215-48d5-b9e2-e215f4920a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Agregar la columna 'class_weight'\n",
    "def add_class_weight(df, label_column=\"is_fraud\"):\n",
    "    try:\n",
    "        total_count = df.count()\n",
    "        fraud_count = df.filter(col(label_column) == 1).count()\n",
    "        non_fraud_count = df.filter(col(label_column) == 0).count()\n",
    "\n",
    "        fraud_weight = total_count / (fraud_count * 2)\n",
    "        non_fraud_weight = total_count / (non_fraud_count * 2)\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"class_weight\",\n",
    "            when(col(label_column) == 1, fraud_weight).otherwise(non_fraud_weight)\n",
    "        )\n",
    "        logger.info(\"Class weights added successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error adding class weights: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fadb5436-cef0-4506-b642-77bc7c1fb81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Model with Cross-Validation and MLflow\n",
    "\n",
    "### **Prop贸sito:**\n",
    "Entrenar un modelo de regresi贸n log铆stica para detectar fraudes en transacciones de tarjetas de cr茅dito, utilizando validaci贸n cruzada para optimizar hiperpar谩metros y rastrear los experimentos en **MLflow**.\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Configuraci贸n de MLflow:**\n",
    "   - Establece el experimento de MLflow para rastrear los par谩metros, m茅tricas y modelos.\n",
    "\n",
    "2. **Definici贸n del modelo:**\n",
    "   - Configura un modelo de regresi贸n log铆stica con las siguientes columnas:\n",
    "     - **`features`**: Caracter铆sticas del conjunto de datos.\n",
    "     - **`is_fraud`**: Etiqueta de clase.\n",
    "     - **`class_weight`**: Pesos para balancear las clases.\n",
    "\n",
    "3. **Cuadr铆cula de hiperpar谩metros:**\n",
    "   - Define una cuadr铆cula para el par谩metro `regParam` con valores `[0.01, 0.1, 1.0]`.\n",
    "\n",
    "4. **Validaci贸n cruzada:**\n",
    "   - Configura un esquema de validaci贸n cruzada con 3 particiones para evaluar las combinaciones de hiperpar谩metros.\n",
    "   - Usa el **rea Bajo la Curva (AUC)** como m茅trica de evaluaci贸n.\n",
    "\n",
    "5. **Entrenamiento del modelo:**\n",
    "   - Ajusta el modelo utilizando validaci贸n cruzada y selecciona el mejor modelo seg煤n la m茅trica de AUC.\n",
    "\n",
    "6. **Registro en MLflow:**\n",
    "   - Registra los par谩metros y el modelo entrenado en MLflow.\n",
    "\n",
    "### **Output:**\n",
    "El modelo de regresi贸n log铆stica entrenado con los hiperpar谩metros 贸ptimos:\n",
    "- **`bestModel`**: El mejor modelo seleccionado basado en AUC.\n",
    "- **Rastreo en MLflow**: Incluye par谩metros, m茅tricas y artefactos del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abe5a01-e79b-4ba3-8ddb-32ea9d934ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Entrenar el modelo con validaci贸n cruzada y MLflow\n",
    "def train_model_with_cv(df, experiment_name):\n",
    "    try:\n",
    "        # Configurar MLflow\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        with mlflow.start_run() as run:\n",
    "            # Configurar el modelo\n",
    "            lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_fraud\", weightCol=\"class_weight\", maxIter=10)\n",
    "\n",
    "            # Configurar la cuadr铆cula de hiperpar谩metros\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "                .build()\n",
    "\n",
    "            # Configurar la validaci贸n cruzada\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"areaUnderROC\")\n",
    "            cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "            # Entrenar el modelo\n",
    "            logger.info(\"Training the model with cross-validation.\")\n",
    "            cvModel = cv.fit(df)\n",
    "\n",
    "            # Registrar el modelo en MLflow\n",
    "            mlflow.log_param(\"RegParam\", [0.01, 0.1, 1.0])\n",
    "            mlflow.spark.log_model(cvModel.bestModel, \"fraud_detection_model\")\n",
    "            logger.info(\"Model logged to MLflow.\")\n",
    "        \n",
    "        return cvModel.bestModel\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during model training: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de69f0b3-ba95-4ada-8229-c7b5aa66dac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluar el Modelo\n",
    "\n",
    "### **Prop贸sito:**\n",
    "Evaluar el rendimiento del modelo entrenado para detectar fraudes utilizando la m茅trica **AUC** (rea Bajo la Curva ROC).\n",
    "\n",
    "### **Proceso:**\n",
    "1. **Generar Predicciones:**\n",
    "   - El modelo entrenado se aplica al conjunto de datos de entrada para generar una columna de predicciones.\n",
    "   \n",
    "2. **Configurar el Evaluador:**\n",
    "   - Se utiliza el evaluador `BinaryClassificationEvaluator` para calcular la m茅trica de **AUC** (rea Bajo la Curva ROC).\n",
    "   \n",
    "3. **Calcular el AUC:**\n",
    "   - El evaluador calcula el AUC en funci贸n de las predicciones generadas por el modelo.\n",
    "   \n",
    "4. **Registrar el Resultado:**\n",
    "   - El valor de AUC se registra en los logs para an谩lisis posterior.\n",
    "\n",
    "### **Output:**\n",
    "- **AUC**: Un valor num茅rico que representa la calidad del modelo en la detecci贸n de fraudes, donde un valor cercano a 1 indica un excelente rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9e54ce-6139-44b1-8140-271744cab7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "def evaluate_model(model, df):\n",
    "    try:\n",
    "        # Generar predicciones\n",
    "        predictions = model.transform(df)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        logger.info(f\"Model evaluation completed. AUC: {auc:.4f}\")\n",
    "        return auc\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during model evaluation: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313dae44-d81b-4f22-aaf9-8b11ffe3ad58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Ejecuci贸n Principal \n",
    "\n",
    "- Configura y ejecuta todo el pipeline de detecci贸n de fraudes.\n",
    "- Carga los datos, agrega pesos para balancear clases, entrena el modelo y eval煤a su rendimiento.\n",
    "- Usa MLflow para rastrear los experimentos y resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f62e20a-d96d-4d07-b6e7-5476f91f207d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/21 22:58:11 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/01/21 22:58:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2662644693673107/1913238ff081446fafc46bc1655e2741/artifacts/fraud_detection_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.3.2', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run puzzled-duck-40 at: https://community.cloud.databricks.com/ml/experiments/2662644693673107/runs/1913238ff081446fafc46bc1655e2741\nИ View experiment at: https://community.cloud.databricks.com/ml/experiments/2662644693673107\nModel evaluation completed. AUC: 0.9911\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuraci贸n inicial\n",
    "        logger.info(\"Starting the fraud detection pipeline.\")\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Rutas y configuraci贸n\n",
    "        delta_path = \"/FileStore/tables/output_delta_table_datapipe_feature_eng_to_train\"\n",
    "        model_path = \"/dbfs/tmp/fraud_detection_cv_model\"\n",
    "        experiment_name = \"/Users/miguel.program.73@gmail.com/fraud_detection_experiment\"\n",
    "        feature_columns = [\n",
    "            \"transaction_velocity\",\n",
    "            \"amount_velocity\",\n",
    "            \"merchant_category_count\",\n",
    "            \"hour_of_day\",\n",
    "            \"day_of_week\"\n",
    "        ]\n",
    "        label_column = \"is_fraud\"\n",
    "\n",
    "        # Cargar y preparar los datos\n",
    "        data = load_and_prepare_data(spark, delta_path, feature_columns, label_column)\n",
    "\n",
    "        # Agregar la columna 'class_weight'\n",
    "        data_with_weights = add_class_weight(data, label_column)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        best_model = train_model_with_cv(data_with_weights, experiment_name)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        auc = evaluate_model(best_model, data_with_weights)\n",
    "        logger.info(f\"Pipeline completed successfully. AUC: {auc:.4f}\")\n",
    "        print(f\"Model evaluation completed. AUC: {auc:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Pipeline execution failed: %s\", str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0777ca5-27c9-44dc-aef5-fe6aa581606f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Descripci贸n de las Pruebas\n",
    "\n",
    "### `test_load_and_prepare_data`\n",
    "- Verifica que la funci贸n cargue los datos desde Delta correctamente.\n",
    "- Asegura que las columnas `features` y `is_fraud` est茅n presentes en el DataFrame.\n",
    "\n",
    "### `test_add_class_weight`\n",
    "- Valida que se genere la columna `class_weight` correctamente.\n",
    "- Verifica que los pesos sean distintos para las clases de fraude y no fraude.\n",
    "\n",
    "### `test_train_model_with_cv`\n",
    "- Comprueba que la funci贸n entrene un modelo de regresi贸n log铆stica utilizando validaci贸n cruzada.\n",
    "- Valida que el modelo resultante sea del tipo `LogisticRegressionModel`.\n",
    "\n",
    "### `test_evaluate_model`\n",
    "- Valida que el modelo pueda generar predicciones.\n",
    "- Verifica que el AUC calculado est茅 dentro del rango v谩lido de 0 a 1.\n",
    "\n",
    "**Nota:** Estas pruebas unitarias, debido a limitaciones de tiempo y a la imposibilidad de ejecutarlas directamente en el mismo notebook, se dejaron de forma hipot茅tica (no las ejecut茅 como tal, pero a煤n asi las hice por el requerimiento). Seg煤n la metodolog铆a de pytest, estas pruebas se ejecutan en archivos `.py`. Por esta raz贸n, decid铆 priorizar otros pasos del proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf85ba85-8012-4d46-b35c-7988dbbf3a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    \"\"\"Crea una sesi贸n de Spark para las pruebas.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"TestFraudDetection\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data(spark):\n",
    "    \"\"\"Crea un DataFrame de prueba.\"\"\"\n",
    "    data = [\n",
    "        (\"T1\", \"C1\", 3, 150.0, 5, 12, 3, 0),\n",
    "        (\"T2\", \"C1\", 1, 100.0, 4, 18, 2, 1),\n",
    "        (\"T3\", \"C2\", 2, 200.0, 3, 14, 4, 0)\n",
    "    ]\n",
    "    schema = [\"transaction_id\", \"customer_id\", \"transaction_velocity\", \"amount_velocity\",\n",
    "              \"merchant_category_count\", \"hour_of_day\", \"day_of_week\", \"is_fraud\"]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "def test_load_and_prepare_data(spark, sample_data):\n",
    "    \"\"\"Prueba la funci贸n load_and_prepare_data.\"\"\"\n",
    "    delta_path = \"/tmp/sample_delta\"\n",
    "    sample_data.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    label_column = \"is_fraud\"\n",
    "\n",
    "    df = load_and_prepare_data(spark, delta_path, feature_columns, label_column)\n",
    "    assert \"features\" in df.columns\n",
    "    assert label_column in df.columns\n",
    "\n",
    "def test_add_class_weight(sample_data):\n",
    "    \"\"\"Prueba la funci贸n add_class_weight.\"\"\"\n",
    "    df = add_class_weight(sample_data, \"is_fraud\")\n",
    "    assert \"class_weight\" in df.columns\n",
    "    fraud_weights = df.filter(df.is_fraud == 1).select(\"class_weight\").distinct().collect()\n",
    "    non_fraud_weights = df.filter(df.is_fraud == 0).select(\"class_weight\").distinct().collect()\n",
    "    assert len(fraud_weights) == 1\n",
    "    assert len(non_fraud_weights) == 1\n",
    "\n",
    "def test_train_model_with_cv(sample_data):\n",
    "    \"\"\"Prueba la funci贸n train_model_with_cv.\"\"\"\n",
    "    experiment_name = \"test_experiment\"\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    sample_data = assembler.transform(sample_data)\n",
    "\n",
    "    df_with_weights = add_class_weight(sample_data, \"is_fraud\")\n",
    "    best_model = train_model_with_cv(df_with_weights, experiment_name)\n",
    "    assert isinstance(best_model, LogisticRegressionModel)\n",
    "\n",
    "def test_evaluate_model(spark, sample_data):\n",
    "    \"\"\"Prueba la funci贸n evaluate_model.\"\"\"\n",
    "    feature_columns = [\"transaction_velocity\", \"amount_velocity\", \"merchant_category_count\", \"hour_of_day\", \"day_of_week\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    df = assembler.transform(sample_data)\n",
    "\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_fraud\", weightCol=\"class_weight\", maxIter=10)\n",
    "    df_with_weights = add_class_weight(df, \"is_fraud\")\n",
    "    model = lr.fit(df_with_weights)\n",
    "\n",
    "    auc = evaluate_model(model, df_with_weights)\n",
    "    assert isinstance(auc, float)\n",
    "    assert 0.0 <= auc <= 1.0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Model Development - Doc & Exp",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
